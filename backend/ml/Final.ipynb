{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lAFAvJ9CTt_A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv('dataset/train.csv')\n",
        "df_test = pd.read_csv('dataset/test.csv')\n",
        "df_holdout = pd.read_csv('dataset/holdout.csv', delimiter=';')\n",
        "\n",
        "df_all = pd.concat([df_train, df_test, df_holdout], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "Qj5LUFWywgbI",
        "outputId": "14d285ff-d8a4-4f11-9742-988d22c6b4bc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11663</th>\n",
              "      <td>sedih etnis tionghoa kena sasaran apalgi scene...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11664</th>\n",
              "      <td>ga tonton film tayang indo</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11665</th>\n",
              "      <td>miskin gak beli</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11666</th>\n",
              "      <td>ewe</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11667</th>\n",
              "      <td>coba dibuka kode</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11668</th>\n",
              "      <td>gue lo es kiko</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11669</th>\n",
              "      <td>mbg audit korupsi terbesar ğŸ˜¢</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11670</th>\n",
              "      <td>ğ•‚ğ•–ğ•Ÿğ•’ğ•¡ğ•’ ğ•™ğ•’ğ•£ğ•¦ğ•¤ ğ•™ğ•¦ğ•¤ğ•“ğ•¦ ğ•˜ğ•¦ğ•– ğ•“ğ•’ğ•Ÿğ•˜ ğŸ˜¢</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11671</th>\n",
              "      <td>syarifairlangga4608 masuk surga</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11672</th>\n",
              "      <td>kalo uang gw 100 juta gw beli figure gojo sato...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 comment  label\n",
              "11663  sedih etnis tionghoa kena sasaran apalgi scene...      0\n",
              "11664                         ga tonton film tayang indo      0\n",
              "11665                                    miskin gak beli      0\n",
              "11666                                                ewe      0\n",
              "11667                                   coba dibuka kode      0\n",
              "11668                                     gue lo es kiko      0\n",
              "11669                       mbg audit korupsi terbesar ğŸ˜¢      0\n",
              "11670                      ğ•‚ğ•–ğ•Ÿğ•’ğ•¡ğ•’ ğ•™ğ•’ğ•£ğ•¦ğ•¤ ğ•™ğ•¦ğ•¤ğ•“ğ•¦ ğ•˜ğ•¦ğ•– ğ•“ğ•’ğ•Ÿğ•˜ ğŸ˜¢      0\n",
              "11671                    syarifairlangga4608 masuk surga      0\n",
              "11672  kalo uang gw 100 juta gw beli figure gojo sato...      0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_all.tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GNHBM6fWwZz9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "class HomoglyphExtractor:\n",
        "    \"\"\"\n",
        "    Ekstraksi kar.akter homoglyph dari dataset\n",
        "    Mendeteksi karakter Unicode yang mirip huruf/angka normal tapi beda encoding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Range Unicode untuk karakter normal (baseline)\n",
        "        self.normal_ranges = [\n",
        "            (0x0020, 0x007E),  # Basic Latin (spasi sampai ~)\n",
        "            (0x00A0, 0x00FF),  # Latin-1 Supplement\n",
        "        ]\n",
        "\n",
        "        # Range Unicode untuk emoji (akan di-exclude)\n",
        "        self.emoji_ranges = [\n",
        "            (0x1F600, 0x1F64F),  # Emoticons\n",
        "            (0x1F300, 0x1F5FF),  # Symbols & Pictographs\n",
        "            (0x1F680, 0x1F6FF),  # Transport & Map\n",
        "            (0x1F1E0, 0x1F1FF),  # Flags\n",
        "            (0x2600, 0x26FF),    # Miscellaneous Symbols\n",
        "            (0x2700, 0x27BF),    # Dingbats\n",
        "            (0xFE00, 0xFE0F),    # Variation Selectors\n",
        "            (0x1F900, 0x1F9FF),  # Supplemental Symbols\n",
        "            (0x1FA70, 0x1FAFF),  # Symbols and Pictographs Extended-A\n",
        "        ]\n",
        "\n",
        "        # Kategori Unicode yang sering digunakan untuk homoglyph\n",
        "        self.suspicious_categories = [\n",
        "            'Lm',  # Letter, Modifier\n",
        "            'Sk',  # Symbol, Modifier\n",
        "            'So',  # Symbol, Other\n",
        "        ]\n",
        "\n",
        "    def is_emoji(self, char):\n",
        "        \"\"\"Cek apakah karakter adalah emoji\"\"\"\n",
        "        code_point = ord(char)\n",
        "        for start, end in self.emoji_ranges:\n",
        "            if start <= code_point <= end:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def is_normal_char(self, char):\n",
        "        \"\"\"Cek apakah karakter adalah ASCII/Latin normal\"\"\"\n",
        "        code_point = ord(char)\n",
        "        for start, end in self.normal_ranges:\n",
        "            if start <= code_point <= end:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def is_homoglyph(self, char):\n",
        "        \"\"\"\n",
        "        Deteksi homoglyph:\n",
        "        - Bukan karakter normal\n",
        "        - Bukan emoji\n",
        "        - Bukan whitespace biasa\n",
        "        - Adalah huruf/angka/simbol yang terlihat mirip normal\n",
        "        \"\"\"\n",
        "        # Skip whitespace biasa\n",
        "        if char in [' ', '\\t', '\\n', '\\r']:\n",
        "            return False\n",
        "\n",
        "        # Skip emoji\n",
        "        if self.is_emoji(char):\n",
        "            return False\n",
        "\n",
        "        # Skip karakter normal\n",
        "        if self.is_normal_char(char):\n",
        "            return False\n",
        "\n",
        "        # Dapatkan kategori Unicode\n",
        "        try:\n",
        "            category = unicodedata.category(char)\n",
        "            name = unicodedata.name(char, '')\n",
        "\n",
        "            # Karakter yang terlihat seperti huruf/angka\n",
        "            # Category: L* (Letter), N* (Number), atau simbol tertentu\n",
        "            is_letter_like = category.startswith('L')\n",
        "            is_number_like = category.startswith('N')\n",
        "            is_suspicious_symbol = category in self.suspicious_categories\n",
        "\n",
        "            # Kata kunci dalam nama Unicode yang mengindikasikan homoglyph\n",
        "            homoglyph_keywords = [\n",
        "                'MATHEMATICAL', 'BOLD', 'ITALIC', 'SCRIPT', 'FRAKTUR',\n",
        "                'DOUBLE-STRUCK', 'SANS-SERIF', 'MONOSPACE',\n",
        "                'FULLWIDTH', 'HALFWIDTH', 'CIRCLED', 'PARENTHESIZED',\n",
        "                'SQUARED', 'NEGATIVE', 'REGIONAL', 'TAG'\n",
        "            ]\n",
        "\n",
        "            has_homoglyph_keyword = any(keyword in name for keyword in homoglyph_keywords)\n",
        "\n",
        "            return (is_letter_like or is_number_like or is_suspicious_symbol) and has_homoglyph_keyword\n",
        "\n",
        "        except (ValueError, TypeError):\n",
        "            # Jika tidak bisa mendapat info Unicode, anggap bukan homoglyph\n",
        "            return False\n",
        "\n",
        "    def extract_homoglyphs_from_text(self, text):\n",
        "        \"\"\"Ekstraksi semua homoglyph dari satu teks\"\"\"\n",
        "        if pd.isna(text) or not isinstance(text, str):\n",
        "            return []\n",
        "\n",
        "        homoglyphs = []\n",
        "        for char in text:\n",
        "            if self.is_homoglyph(char):\n",
        "                try:\n",
        "                    name = unicodedata.name(char, 'UNKNOWN')\n",
        "                    code_point = f\"U+{ord(char):04X}\"\n",
        "                    homoglyphs.append({\n",
        "                        'char': char,\n",
        "                        'unicode_name': name,\n",
        "                        'code_point': code_point,\n",
        "                        'category': unicodedata.category(char)\n",
        "                    })\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        return homoglyphs\n",
        "\n",
        "    def analyze_dataset(self, df, text_column='comment'):\n",
        "        \"\"\"\n",
        "        Analisis dataset dan ekstraksi semua homoglyph yang ditemukan\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pandas DataFrame\n",
        "            Dataset dengan kolom teks\n",
        "        text_column : str\n",
        "            Nama kolom yang berisi teks (default: 'comment')\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict : Hasil analisis lengkap\n",
        "        \"\"\"\n",
        "        print(f\"Menganalisis kolom '{text_column}'...\")\n",
        "        print(f\"Total rows: {len(df)}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        all_homoglyphs = []\n",
        "        rows_with_homoglyphs = []\n",
        "\n",
        "        # Iterasi setiap baris\n",
        "        for idx, row in df.iterrows():\n",
        "            text = row[text_column]\n",
        "            homoglyphs = self.extract_homoglyphs_from_text(text)\n",
        "\n",
        "            if homoglyphs:\n",
        "                rows_with_homoglyphs.append({\n",
        "                    'index': idx,\n",
        "                    'text': text,\n",
        "                    'homoglyphs': homoglyphs,\n",
        "                    'homoglyph_count': len(homoglyphs)\n",
        "                })\n",
        "                all_homoglyphs.extend(homoglyphs)\n",
        "\n",
        "        # Statistik homoglyph\n",
        "        homoglyph_chars = [h['char'] for h in all_homoglyphs]\n",
        "        homoglyph_counter = Counter(homoglyph_chars)\n",
        "\n",
        "        # Buat mapping untuk normalisasi\n",
        "        homoglyph_mapping = self._create_normalization_mapping(all_homoglyphs)\n",
        "\n",
        "        results = {\n",
        "            'total_rows': len(df),\n",
        "            'rows_with_homoglyphs': len(rows_with_homoglyphs),\n",
        "            'total_homoglyph_chars': len(all_homoglyphs),\n",
        "            'unique_homoglyphs': len(homoglyph_counter),\n",
        "            'homoglyph_frequency': homoglyph_counter,\n",
        "            'detailed_rows': rows_with_homoglyphs,\n",
        "            'normalization_mapping': homoglyph_mapping\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _create_normalization_mapping(self, homoglyphs):\n",
        "        \"\"\"\n",
        "        Buat mapping otomatis dari homoglyph ke karakter normal\n",
        "        Berdasarkan nama Unicode\n",
        "        \"\"\"\n",
        "        mapping = {}\n",
        "\n",
        "        for h in homoglyphs:\n",
        "            char = h['char']\n",
        "            name = h['unicode_name']\n",
        "\n",
        "            if char in mapping:\n",
        "                continue\n",
        "\n",
        "            # Ekstraksi karakter normal dari nama Unicode\n",
        "            # Contoh: \"MATHEMATICAL BOLD CAPITAL A\" -> \"A\"\n",
        "            normal_char = self._extract_normal_char_from_name(name)\n",
        "            if normal_char:\n",
        "                mapping[char] = normal_char\n",
        "\n",
        "        return mapping\n",
        "\n",
        "    def _extract_normal_char_from_name(self, unicode_name):\n",
        "        \"\"\"Ekstraksi karakter normal dari nama Unicode\"\"\"\n",
        "        # Pattern untuk huruf kapital\n",
        "        if 'CAPITAL' in unicode_name or 'UPPER' in unicode_name:\n",
        "            # Cari huruf A-Z di akhir nama\n",
        "            match = re.search(r'\\b([A-Z])\\b', unicode_name[::-1])\n",
        "            if match:\n",
        "                return match.group(1)\n",
        "\n",
        "        # Pattern untuk huruf kecil\n",
        "        if 'SMALL' in unicode_name or 'LOWER' in unicode_name:\n",
        "            # Map ke huruf kecil\n",
        "            match = re.search(r'\\b([A-Z])\\b', unicode_name[::-1])\n",
        "            if match:\n",
        "                return match.group(1).lower()\n",
        "\n",
        "        # Pattern untuk angka\n",
        "        if 'DIGIT' in unicode_name:\n",
        "            match = re.search(r'DIGIT (\\w+)', unicode_name)\n",
        "            if match:\n",
        "                digit_name = match.group(1)\n",
        "                digit_map = {\n",
        "                    'ZERO': '0', 'ONE': '1', 'TWO': '2', 'THREE': '3',\n",
        "                    'FOUR': '4', 'FIVE': '5', 'SIX': '6', 'SEVEN': '7',\n",
        "                    'EIGHT': '8', 'NINE': '9'\n",
        "                }\n",
        "                return digit_map.get(digit_name)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def print_summary(self, results):\n",
        "        \"\"\"Cetak ringkasan hasil analisis\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"RINGKASAN ANALISIS HOMOGLYPH\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(f\"\\nğŸ“Š Statistik:\")\n",
        "        print(f\"  - Total baris dalam dataset: {results['total_rows']}\")\n",
        "        print(f\"  - Baris yang mengandung homoglyph: {results['rows_with_homoglyphs']}\")\n",
        "        print(f\"  - Persentase: {results['rows_with_homoglyphs']/results['total_rows']*100:.2f}%\")\n",
        "        print(f\"  - Total karakter homoglyph ditemukan: {results['total_homoglyph_chars']}\")\n",
        "        print(f\"  - Unique homoglyph characters: {results['unique_homoglyphs']}\")\n",
        "\n",
        "        print(f\"\\nğŸ”¤ Top 10 Homoglyph Paling Sering Muncul:\")\n",
        "        for char, count in results['homoglyph_frequency'].most_common(10):\n",
        "            try:\n",
        "                name = unicodedata.name(char, 'UNKNOWN')\n",
        "                code = f\"U+{ord(char):04X}\"\n",
        "                normal = results['normalization_mapping'].get(char, '?')\n",
        "                print(f\"  '{char}' â†’ '{normal}'  |  {code}  |  {count}x  |  {name}\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        print(f\"\\nğŸ“ Contoh Komentar dengan Homoglyph (5 pertama):\")\n",
        "        for i, row in enumerate(results['detailed_rows'][:5], 1):\n",
        "            print(f\"\\n  [{i}] Index: {row['index']}\")\n",
        "            print(f\"      Text: {row['text'][:100]}{'...' if len(row['text']) > 100 else ''}\")\n",
        "            print(f\"      Homoglyphs found: {row['homoglyph_count']}\")\n",
        "            unique_chars = list(set([h['char'] for h in row['homoglyphs']]))\n",
        "            print(f\"      Characters: {', '.join(unique_chars)}\")\n",
        "\n",
        "    def export_mapping_code(self, results, output_file='homoglyph_mapping.py'):\n",
        "        \"\"\"\n",
        "        Export mapping ke file Python yang bisa langsung digunakan\n",
        "        \"\"\"\n",
        "        mapping = results['normalization_mapping']\n",
        "\n",
        "        code = \"# Auto-generated homoglyph mapping\\n\"\n",
        "        code += \"# Generated from dataset analysis\\n\\n\"\n",
        "        code += \"HOMOGLYPH_MAP = {\\n\"\n",
        "\n",
        "        for homo, normal in sorted(mapping.items()):\n",
        "            try:\n",
        "                name = unicodedata.name(homo, 'UNKNOWN')\n",
        "                code += f\"    '{homo}': '{normal}',  # {name}\\n\"\n",
        "            except:\n",
        "                code += f\"    '{homo}': '{normal}',\\n\"\n",
        "\n",
        "        code += \"}\\n\\n\"\n",
        "        code += \"def normalize_homoglyph(text):\\n\"\n",
        "        code += \"    \\\"\\\"\\\"Normalize homoglyph characters to normal ASCII\\\"\\\"\\\"\\n\"\n",
        "        code += \"    for homo, normal in HOMOGLYPH_MAP.items():\\n\"\n",
        "        code += \"        text = text.replace(homo, normal)\\n\"\n",
        "        code += \"    return text\\n\"\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(code)\n",
        "\n",
        "        print(f\"\\nâœ… Mapping code exported to: {output_file}\")\n",
        "        return code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Km87FgWwsEJ",
        "outputId": "1b3aa1e6-5930-4488-adbc-c7ee930e8f64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Menganalisis kolom 'comment'...\n",
            "Total rows: 11673\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "extractor = HomoglyphExtractor()\n",
        "\n",
        "# Analisis dataset\n",
        "results = extractor.analyze_dataset(df_all, text_column='comment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-OXTLf8wulB",
        "outputId": "d373408d-e6ee-4927-e96a-0ce61755b95a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "RINGKASAN ANALISIS HOMOGLYPH\n",
            "================================================================================\n",
            "\n",
            "ğŸ“Š Statistik:\n",
            "  - Total baris dalam dataset: 11673\n",
            "  - Baris yang mengandung homoglyph: 927\n",
            "  - Persentase: 7.94%\n",
            "  - Total karakter homoglyph ditemukan: 5715\n",
            "  - Unique homoglyph characters: 217\n",
            "\n",
            "ğŸ”¤ Top 10 Homoglyph Paling Sering Muncul:\n",
            "  'ğŸ´' â†’ '8'  |  U+1D7F4  |  345x  |  MATHEMATICAL SANS-SERIF BOLD DIGIT EIGHT\n",
            "  'ğ—§' â†’ 'T'  |  U+1D5E7  |  209x  |  MATHEMATICAL SANS-SERIF BOLD CAPITAL T\n",
            "  'ğ€' â†’ 'A'  |  U+1D400  |  196x  |  MATHEMATICAL BOLD CAPITAL A\n",
            "  'ğ˜¼' â†’ 'A'  |  U+1D63C  |  176x  |  MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL A\n",
            "  'ğ—¢' â†’ 'O'  |  U+1D5E2  |  176x  |  MATHEMATICAL SANS-SERIF BOLD CAPITAL O\n",
            "  'ğŸ–' â†’ '8'  |  U+1D7D6  |  168x  |  MATHEMATICAL BOLD DIGIT EIGHT\n",
            "  'ğ—¨' â†’ 'U'  |  U+1D5E8  |  140x  |  MATHEMATICAL SANS-SERIF BOLD CAPITAL U\n",
            "  'ğƒ' â†’ 'D'  |  U+1D403  |  136x  |  MATHEMATICAL BOLD CAPITAL D\n",
            "  'ğŸ©' â†’ '7'  |  U+1D7E9  |  125x  |  MATHEMATICAL SANS-SERIF DIGIT SEVEN\n",
            "  'ğ˜–' â†’ 'O'  |  U+1D616  |  124x  |  MATHEMATICAL SANS-SERIF ITALIC CAPITAL O\n",
            "\n",
            "ğŸ“ Contoh Komentar dengan Homoglyph (5 pertama):\n",
            "\n",
            "  [1] Index: 27\n",
            "      Text: ğƒ ğ ğ™ ğ˜ˆ ğŸ½ 7 emang gachor parah\n",
            "      Homoglyphs found: 5\n",
            "      Characters: ğƒ, ğ˜ˆ, ğ™, ğ, ğŸ½\n",
            "\n",
            "  [2] Index: 33\n",
            "      Text: main d ğ¸ ğ– ğ€ d ğ‘‚ ğ‘… a bikin hariku menyenangkan terima kasih ğŸ\n",
            "      Homoglyphs found: 5\n",
            "      Characters: ğ¸, ğ‘…, ğ‘‚, ğ–, ğ€\n",
            "\n",
            "  [3] Index: 43\n",
            "      Text: 225 langsung ğ’ğ†ğˆğŸ–ğŸ– bang ğŸ˜¹ğŸ˜¹\n",
            "      Homoglyphs found: 5\n",
            "      Characters: ğŸ–, ğ†, ğ’, ğˆ\n",
            "\n",
            "  [4] Index: 58\n",
            "      Text: mencari platform mendorong pertumbuhan ğ˜¿ ğ„ w a ğ˜‹ Ğ¾ ğ‘… Ğ°layak dicoba ğŸ­\n",
            "      Homoglyphs found: 4\n",
            "      Characters: ğ‘…, ğ˜¿, ğ„, ğ˜‹\n",
            "\n",
            "  [5] Index: 71\n",
            "      Text: makasih ğŠğ—¨ğ—¦ğ—¨ğ— ğ—”ğ—§ğŸğ—§ğŸuda ngasih ğŸ” âœ…\n",
            "      Homoglyphs found: 10\n",
            "      Characters: ğ—¨, ğŸ, ğ— , ğ—¦, ğŠ, ğ—§, ğ—”\n",
            "\n",
            "âœ… Mapping code exported to: homoglyph_mapping.py\n"
          ]
        }
      ],
      "source": [
        "extractor.print_summary(results)\n",
        "\n",
        "# Export mapping ke file Python\n",
        "mapping_code = extractor.export_mapping_code(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-Q3EZzPxaXJ"
      },
      "source": [
        "Homoglyph Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4lutN-t7xdfp"
      },
      "outputs": [],
      "source": [
        "# Auto-generated homoglyph mapping\n",
        "# Generated from dataset analysis\n",
        "\n",
        "HOMOGLYPH_MAP = {\n",
        "    'â„': 'H',  # DOUBLE-STRUCK CAPITAL H\n",
        "    'ï¼‘': '1',  # FULLWIDTH DIGIT ONE\n",
        "    'ï¼“': '3',  # FULLWIDTH DIGIT THREE\n",
        "    'ï¼˜': '8',  # FULLWIDTH DIGIT EIGHT\n",
        "    'ï½': 'a',  # FULLWIDTH LATIN SMALL LETTER A\n",
        "    'ï½„': 'd',  # FULLWIDTH LATIN SMALL LETTER D\n",
        "    'ï½…': 'e',  # FULLWIDTH LATIN SMALL LETTER E\n",
        "    'ï½ˆ': 'h',  # FULLWIDTH LATIN SMALL LETTER H\n",
        "    'ï½‰': 'i',  # FULLWIDTH LATIN SMALL LETTER I\n",
        "    'ï½Š': 'j',  # FULLWIDTH LATIN SMALL LETTER J\n",
        "    'ï½Œ': 'l',  # FULLWIDTH LATIN SMALL LETTER L\n",
        "    'ï½': 'n',  # FULLWIDTH LATIN SMALL LETTER N\n",
        "    'ï½': 'o',  # FULLWIDTH LATIN SMALL LETTER O\n",
        "    'ï½': 'p',  # FULLWIDTH LATIN SMALL LETTER P\n",
        "    'ï½“': 's',  # FULLWIDTH LATIN SMALL LETTER S\n",
        "    'ï½”': 't',  # FULLWIDTH LATIN SMALL LETTER T\n",
        "    'ï½•': 'u',  # FULLWIDTH LATIN SMALL LETTER U\n",
        "    'ï½—': 'w',  # FULLWIDTH LATIN SMALL LETTER W\n",
        "    'ğ€': 'A',  # MATHEMATICAL BOLD CAPITAL A\n",
        "    'ğ': 'B',  # MATHEMATICAL BOLD CAPITAL B\n",
        "    'ğƒ': 'D',  # MATHEMATICAL BOLD CAPITAL D\n",
        "    'ğ„': 'E',  # MATHEMATICAL BOLD CAPITAL E\n",
        "    'ğ†': 'G',  # MATHEMATICAL BOLD CAPITAL G\n",
        "    'ğ‡': 'H',  # MATHEMATICAL BOLD CAPITAL H\n",
        "    'ğˆ': 'I',  # MATHEMATICAL BOLD CAPITAL I\n",
        "    'ğŠ': 'K',  # MATHEMATICAL BOLD CAPITAL K\n",
        "    'ğ‹': 'L',  # MATHEMATICAL BOLD CAPITAL L\n",
        "    'ğŒ': 'M',  # MATHEMATICAL BOLD CAPITAL M\n",
        "    'ğ': 'N',  # MATHEMATICAL BOLD CAPITAL N\n",
        "    'ğ': 'O',  # MATHEMATICAL BOLD CAPITAL O\n",
        "    'ğ‘': 'R',  # MATHEMATICAL BOLD CAPITAL R\n",
        "    'ğ’': 'S',  # MATHEMATICAL BOLD CAPITAL S\n",
        "    'ğ“': 'T',  # MATHEMATICAL BOLD CAPITAL T\n",
        "    'ğ”': 'U',  # MATHEMATICAL BOLD CAPITAL U\n",
        "    'ğ•': 'V',  # MATHEMATICAL BOLD CAPITAL V\n",
        "    'ğ–': 'W',  # MATHEMATICAL BOLD CAPITAL W\n",
        "    'ğ—': 'X',  # MATHEMATICAL BOLD CAPITAL X\n",
        "    'ğ˜': 'Y',  # MATHEMATICAL BOLD CAPITAL Y\n",
        "    'ğš': 'a',  # MATHEMATICAL BOLD SMALL A\n",
        "    'ğ›': 'b',  # MATHEMATICAL BOLD SMALL B\n",
        "    'ğ': 'd',  # MATHEMATICAL BOLD SMALL D\n",
        "    'ğ': 'e',  # MATHEMATICAL BOLD SMALL E\n",
        "    'ğ ': 'g',  # MATHEMATICAL BOLD SMALL G\n",
        "    'ğ¢': 'i',  # MATHEMATICAL BOLD SMALL I\n",
        "    'ğ£': 'j',  # MATHEMATICAL BOLD SMALL J\n",
        "    'ğ¥': 'l',  # MATHEMATICAL BOLD SMALL L\n",
        "    'ğ§': 'n',  # MATHEMATICAL BOLD SMALL N\n",
        "    'ğ¨': 'o',  # MATHEMATICAL BOLD SMALL O\n",
        "    'ğ«': 'r',  # MATHEMATICAL BOLD SMALL R\n",
        "    'ğ¬': 's',  # MATHEMATICAL BOLD SMALL S\n",
        "    'ğ­': 't',  # MATHEMATICAL BOLD SMALL T\n",
        "    'ğ²': 'y',  # MATHEMATICAL BOLD SMALL Y\n",
        "    'ğ´': 'A',  # MATHEMATICAL ITALIC CAPITAL A\n",
        "    'ğ·': 'D',  # MATHEMATICAL ITALIC CAPITAL D\n",
        "    'ğ¸': 'E',  # MATHEMATICAL ITALIC CAPITAL E\n",
        "    'ğº': 'G',  # MATHEMATICAL ITALIC CAPITAL G\n",
        "    'ğ»': 'H',  # MATHEMATICAL ITALIC CAPITAL H\n",
        "    'ğ¼': 'I',  # MATHEMATICAL ITALIC CAPITAL I\n",
        "    'ğ¿': 'L',  # MATHEMATICAL ITALIC CAPITAL L\n",
        "    'ğ‘€': 'M',  # MATHEMATICAL ITALIC CAPITAL M\n",
        "    'ğ‘‚': 'O',  # MATHEMATICAL ITALIC CAPITAL O\n",
        "    'ğ‘…': 'R',  # MATHEMATICAL ITALIC CAPITAL R\n",
        "    'ğ‘†': 'S',  # MATHEMATICAL ITALIC CAPITAL S\n",
        "    'ğ‘‡': 'T',  # MATHEMATICAL ITALIC CAPITAL T\n",
        "    'ğ‘ˆ': 'U',  # MATHEMATICAL ITALIC CAPITAL U\n",
        "    'ğ‘Š': 'W',  # MATHEMATICAL ITALIC CAPITAL W\n",
        "    'ğ‘‹': 'X',  # MATHEMATICAL ITALIC CAPITAL X\n",
        "    'ğ‘¨': 'A',  # MATHEMATICAL BOLD ITALIC CAPITAL A\n",
        "    'ğ‘ª': 'C',  # MATHEMATICAL BOLD ITALIC CAPITAL C\n",
        "    'ğ‘«': 'D',  # MATHEMATICAL BOLD ITALIC CAPITAL D\n",
        "    'ğ‘®': 'G',  # MATHEMATICAL BOLD ITALIC CAPITAL G\n",
        "    'ğ‘°': 'I',  # MATHEMATICAL BOLD ITALIC CAPITAL I\n",
        "    'ğ‘²': 'K',  # MATHEMATICAL BOLD ITALIC CAPITAL K\n",
        "    'ğ‘³': 'L',  # MATHEMATICAL BOLD ITALIC CAPITAL L\n",
        "    'ğ‘´': 'M',  # MATHEMATICAL BOLD ITALIC CAPITAL M\n",
        "    'ğ‘µ': 'N',  # MATHEMATICAL BOLD ITALIC CAPITAL N\n",
        "    'ğ‘¶': 'O',  # MATHEMATICAL BOLD ITALIC CAPITAL O\n",
        "    'ğ‘º': 'S',  # MATHEMATICAL BOLD ITALIC CAPITAL S\n",
        "    'ğ‘»': 'T',  # MATHEMATICAL BOLD ITALIC CAPITAL T\n",
        "    'ğ’€': 'Y',  # MATHEMATICAL BOLD ITALIC CAPITAL Y\n",
        "    'ğ’œ': 'A',  # MATHEMATICAL SCRIPT CAPITAL A\n",
        "    'ğ’¢': 'G',  # MATHEMATICAL SCRIPT CAPITAL G\n",
        "    'ğ’©': 'N',  # MATHEMATICAL SCRIPT CAPITAL N\n",
        "    'ğ’«': 'P',  # MATHEMATICAL SCRIPT CAPITAL P\n",
        "    'ğ’®': 'S',  # MATHEMATICAL SCRIPT CAPITAL S\n",
        "    'ğ’¯': 'T',  # MATHEMATICAL SCRIPT CAPITAL T\n",
        "    'ğ’°': 'U',  # MATHEMATICAL SCRIPT CAPITAL U\n",
        "    'ğ“': 'A',  # MATHEMATICAL BOLD SCRIPT CAPITAL A\n",
        "    'ğ“˜': 'I',  # MATHEMATICAL BOLD SCRIPT CAPITAL I\n",
        "    'ğ“›': 'L',  # MATHEMATICAL BOLD SCRIPT CAPITAL L\n",
        "    'ğ“': 'N',  # MATHEMATICAL BOLD SCRIPT CAPITAL N\n",
        "    'ğ“Ÿ': 'P',  # MATHEMATICAL BOLD SCRIPT CAPITAL P\n",
        "    'ğ“¤': 'U',  # MATHEMATICAL BOLD SCRIPT CAPITAL U\n",
        "    'ğ“¦': 'W',  # MATHEMATICAL BOLD SCRIPT CAPITAL W\n",
        "    'ğ•‚': 'K',  # MATHEMATICAL DOUBLE-STRUCK CAPITAL K\n",
        "    'ğ•’': 'a',  # MATHEMATICAL DOUBLE-STRUCK SMALL A\n",
        "    'ğ•“': 'b',  # MATHEMATICAL DOUBLE-STRUCK SMALL B\n",
        "    'ğ••': 'd',  # MATHEMATICAL DOUBLE-STRUCK SMALL D\n",
        "    'ğ•–': 'e',  # MATHEMATICAL DOUBLE-STRUCK SMALL E\n",
        "    'ğ•˜': 'g',  # MATHEMATICAL DOUBLE-STRUCK SMALL G\n",
        "    'ğ•™': 'h',  # MATHEMATICAL DOUBLE-STRUCK SMALL H\n",
        "    'ğ•š': 'i',  # MATHEMATICAL DOUBLE-STRUCK SMALL I\n",
        "    'ğ•Ÿ': 'n',  # MATHEMATICAL DOUBLE-STRUCK SMALL N\n",
        "    'ğ•¡': 'p',  # MATHEMATICAL DOUBLE-STRUCK SMALL P\n",
        "    'ğ•£': 'r',  # MATHEMATICAL DOUBLE-STRUCK SMALL R\n",
        "    'ğ•¤': 's',  # MATHEMATICAL DOUBLE-STRUCK SMALL S\n",
        "    'ğ•¦': 'u',  # MATHEMATICAL DOUBLE-STRUCK SMALL U\n",
        "    'ğ–ª': 'K',  # MATHEMATICAL SANS-SERIF CAPITAL K\n",
        "    'ğ–º': 'a',  # MATHEMATICAL SANS-SERIF SMALL A\n",
        "    'ğ–¾': 'e',  # MATHEMATICAL SANS-SERIF SMALL E\n",
        "    'ğ—€': 'g',  # MATHEMATICAL SANS-SERIF SMALL G\n",
        "    'ğ—': 'h',  # MATHEMATICAL SANS-SERIF SMALL H\n",
        "    'ğ—‚': 'i',  # MATHEMATICAL SANS-SERIF SMALL I\n",
        "    'ğ—ƒ': 'j',  # MATHEMATICAL SANS-SERIF SMALL J\n",
        "    'ğ—„': 'k',  # MATHEMATICAL SANS-SERIF SMALL K\n",
        "    'ğ—…': 'l',  # MATHEMATICAL SANS-SERIF SMALL L\n",
        "    'ğ—†': 'm',  # MATHEMATICAL SANS-SERIF SMALL M\n",
        "    'ğ—‡': 'n',  # MATHEMATICAL SANS-SERIF SMALL N\n",
        "    'ğ—ˆ': 'o',  # MATHEMATICAL SANS-SERIF SMALL O\n",
        "    'ğ—': 't',  # MATHEMATICAL SANS-SERIF SMALL T\n",
        "    'ğ—': 'u',  # MATHEMATICAL SANS-SERIF SMALL U\n",
        "    'ğ—’': 'y',  # MATHEMATICAL SANS-SERIF SMALL Y\n",
        "    'ğ—”': 'A',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL A\n",
        "    'ğ—•': 'B',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL B\n",
        "    'ğ—–': 'C',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL C\n",
        "    'ğ——': 'D',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL D\n",
        "    'ğ—˜': 'E',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL E\n",
        "    'ğ—š': 'G',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL G\n",
        "    'ğ—›': 'H',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL H\n",
        "    'ğ—œ': 'I',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL I\n",
        "    'ğ—': 'J',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL J\n",
        "    'ğ—': 'K',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL K\n",
        "    'ğ—Ÿ': 'L',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL L\n",
        "    'ğ— ': 'M',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL M\n",
        "    'ğ—¡': 'N',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL N\n",
        "    'ğ—¢': 'O',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL O\n",
        "    'ğ—£': 'P',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL P\n",
        "    'ğ—¥': 'R',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL R\n",
        "    'ğ—¦': 'S',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL S\n",
        "    'ğ—§': 'T',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL T\n",
        "    'ğ—¨': 'U',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL U\n",
        "    'ğ—ª': 'W',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL W\n",
        "    'ğ—¬': 'Y',  # MATHEMATICAL SANS-SERIF BOLD CAPITAL Y\n",
        "    'ğ—®': 'a',  # MATHEMATICAL SANS-SERIF BOLD SMALL A\n",
        "    'ğ—²': 'e',  # MATHEMATICAL SANS-SERIF BOLD SMALL E\n",
        "    'ğ—´': 'g',  # MATHEMATICAL SANS-SERIF BOLD SMALL G\n",
        "    'ğ—¶': 'i',  # MATHEMATICAL SANS-SERIF BOLD SMALL I\n",
        "    'ğ—¹': 'l',  # MATHEMATICAL SANS-SERIF BOLD SMALL L\n",
        "    'ğ—º': 'm',  # MATHEMATICAL SANS-SERIF BOLD SMALL M\n",
        "    'ğ—»': 'n',  # MATHEMATICAL SANS-SERIF BOLD SMALL N\n",
        "    'ğ—¼': 'o',  # MATHEMATICAL SANS-SERIF BOLD SMALL O\n",
        "    'ğ—¿': 'r',  # MATHEMATICAL SANS-SERIF BOLD SMALL R\n",
        "    'ğ˜€': 's',  # MATHEMATICAL SANS-SERIF BOLD SMALL S\n",
        "    'ğ˜‚': 'u',  # MATHEMATICAL SANS-SERIF BOLD SMALL U\n",
        "    'ğ˜„': 'w',  # MATHEMATICAL SANS-SERIF BOLD SMALL W\n",
        "    'ğ˜†': 'y',  # MATHEMATICAL SANS-SERIF BOLD SMALL Y\n",
        "    'ğ˜ˆ': 'A',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL A\n",
        "    'ğ˜‹': 'D',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL D\n",
        "    'ğ˜Œ': 'E',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL E\n",
        "    'ğ˜': 'G',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL G\n",
        "    'ğ˜': 'H',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL H\n",
        "    'ğ˜“': 'L',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL L\n",
        "    'ğ˜”': 'M',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL M\n",
        "    'ğ˜–': 'O',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL O\n",
        "    'ğ˜™': 'R',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL R\n",
        "    'ğ˜š': 'S',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL S\n",
        "    'ğ˜›': 'T',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL T\n",
        "    'ğ˜œ': 'U',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL U\n",
        "    'ğ˜': 'W',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL W\n",
        "    'ğ˜Ÿ': 'X',  # MATHEMATICAL SANS-SERIF ITALIC CAPITAL X\n",
        "    'ğ˜¦': 'e',  # MATHEMATICAL SANS-SERIF ITALIC SMALL E\n",
        "    'ğ˜¬': 'k',  # MATHEMATICAL SANS-SERIF ITALIC SMALL K\n",
        "    'ğ˜¶': 'u',  # MATHEMATICAL SANS-SERIF ITALIC SMALL U\n",
        "    'ğ˜¼': 'A',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL A\n",
        "    'ğ˜½': 'B',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL B\n",
        "    'ğ˜¿': 'D',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL D\n",
        "    'ğ™€': 'E',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL E\n",
        "    'ğ™': 'F',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL F\n",
        "    'ğ™‚': 'G',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL G\n",
        "    'ğ™ƒ': 'H',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL H\n",
        "    'ğ™„': 'I',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL I\n",
        "    'ğ™†': 'K',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL K\n",
        "    'ğ™‡': 'L',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL L\n",
        "    'ğ™ˆ': 'M',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL M\n",
        "    'ğ™‰': 'N',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL N\n",
        "    'ğ™Š': 'O',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL O\n",
        "    'ğ™‹': 'P',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL P\n",
        "    'ğ™': 'R',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL R\n",
        "    'ğ™': 'S',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL S\n",
        "    'ğ™': 'T',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL T\n",
        "    'ğ™': 'U',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL U\n",
        "    'ğ™’': 'W',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL W\n",
        "    'ğ™“': 'X',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL X\n",
        "    'ğ™”': 'Y',  # MATHEMATICAL SANS-SERIF BOLD ITALIC CAPITAL Y\n",
        "    'ğ™¶': 'G',  # MATHEMATICAL MONOSPACE CAPITAL G\n",
        "    'ğšŠ': 'a',  # MATHEMATICAL MONOSPACE SMALL A\n",
        "    'ğš‹': 'b',  # MATHEMATICAL MONOSPACE SMALL B\n",
        "    'ğš': 't',  # MATHEMATICAL MONOSPACE SMALL T\n",
        "    'ğš': 'u',  # MATHEMATICAL MONOSPACE SMALL U\n",
        "    'ğŸ': '0',  # MATHEMATICAL BOLD DIGIT ZERO\n",
        "    'ğŸ': '1',  # MATHEMATICAL BOLD DIGIT ONE\n",
        "    'ğŸ': '2',  # MATHEMATICAL BOLD DIGIT TWO\n",
        "    'ğŸ‘': '3',  # MATHEMATICAL BOLD DIGIT THREE\n",
        "    'ğŸ’': '4',  # MATHEMATICAL BOLD DIGIT FOUR\n",
        "    'ğŸ•': '7',  # MATHEMATICAL BOLD DIGIT SEVEN\n",
        "    'ğŸ–': '8',  # MATHEMATICAL BOLD DIGIT EIGHT\n",
        "    'ğŸ©': '7',  # MATHEMATICAL SANS-SERIF DIGIT SEVEN\n",
        "    'ğŸª': '8',  # MATHEMATICAL SANS-SERIF DIGIT EIGHT\n",
        "    'ğŸ®': '2',  # MATHEMATICAL SANS-SERIF BOLD DIGIT TWO\n",
        "    'ğŸ³': '7',  # MATHEMATICAL SANS-SERIF BOLD DIGIT SEVEN\n",
        "    'ğŸ´': '8',  # MATHEMATICAL SANS-SERIF BOLD DIGIT EIGHT\n",
        "    'ğŸµ': '9',  # MATHEMATICAL SANS-SERIF BOLD DIGIT NINE\n",
        "    'ğŸ½': '7',  # MATHEMATICAL MONOSPACE DIGIT SEVEN\n",
        "    'ğŸ¾': '8',  # MATHEMATICAL MONOSPACE DIGIT EIGHT\n",
        "}\n",
        "\n",
        "def normalize_homoglyph(text):\n",
        "    \"\"\"Normalize homoglyph characters to normal ASCII\"\"\"\n",
        "    for homo, normal in HOMOGLYPH_MAP.items():\n",
        "        text = text.replace(homo, normal)\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "BR--9Mc_1a-9",
        "outputId": "3da24c9a-ae1a-409d-9ff5-32f2f84cf989"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in ./.venv/lib/python3.13/site-packages (from gensim) (2.3.3)\n",
            "Requirement already satisfied: scipy>=1.7.0 in ./.venv/lib/python3.13/site-packages (from gensim) (1.16.2)\n",
            "Collecting smart_open>=1.8.1 (from gensim)\n",
            "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: wrapt in ./.venv/lib/python3.13/site-packages (from smart_open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.4.0-cp313-cp313-macosx_11_0_arm64.whl (24.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.4/24.4 MB\u001b[0m \u001b[31m748.0 kB/s\u001b[0m  \u001b[33m0:00:32\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
            "Installing collected packages: smart_open, gensim\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/2\u001b[0m [gensim]2m1/2\u001b[0m [gensim]\n",
            "\u001b[1A\u001b[2KSuccessfully installed gensim-4.4.0 smart_open-7.5.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mh5Xbv24w6lS"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# from xgboost import XGBClassifier\n",
        "# from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "import re\n",
        "import unicodedata\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================\n",
        "# PREPROCESSING & FEATURE ENGINEERING\n",
        "# ============================================================\n",
        "\n",
        "class TextPreprocessor:\n",
        "    \"\"\"Preprocessing untuk menangani homoglyph dan variasi Unicode\"\"\"\n",
        "    def normalize_homoglyph(self, text):\n",
        "        \"\"\"Konversi homoglyph Unicode ke karakter normal\"\"\"\n",
        "        for homo, normal in HOMOGLYPH_MAP.items():\n",
        "            text = text.replace(homo, normal)\n",
        "        return text\n",
        "\n",
        "    def normalize_unicode(self, text):\n",
        "        \"\"\"Normalisasi Unicode menggunakan NFKD\"\"\"\n",
        "        return unicodedata.normalize('NFKD', text)\n",
        "\n",
        "    def remove_extra_spaces(self, text):\n",
        "        \"\"\"Hapus spasi berlebih\"\"\"\n",
        "        return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        \"\"\"Pipeline preprocessing lengkap\"\"\"\n",
        "        text = str(text).lower()\n",
        "        text = self.normalize_homoglyph(text)\n",
        "        text = self.normalize_unicode(text)\n",
        "        text = self.remove_extra_spaces(text)\n",
        "        return text\n",
        "\n",
        "\n",
        "class AdditionalFeatures:\n",
        "    \"\"\"Ekstraksi fitur tambahan untuk deteksi spam\"\"\"\n",
        "\n",
        "    def count_emoji(self, text):\n",
        "        \"\"\"Hitung jumlah emoji\"\"\"\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "            u\"\\U00002702-\\U000027B0\"\n",
        "            u\"\\U000024C2-\\U0001F251\"\n",
        "            \"]+\", flags=re.UNICODE)\n",
        "        return len(emoji_pattern.findall(text))\n",
        "\n",
        "    def capital_ratio(self, text):\n",
        "        \"\"\"Rasio huruf kapital\"\"\"\n",
        "        if len(text) == 0:\n",
        "            return 0\n",
        "        return sum(1 for c in text if c.isupper()) / len(text)\n",
        "\n",
        "    def has_numbers_in_word(self, text):\n",
        "        \"\"\"Deteksi angka dalam kata (SLOT88, PLUTO88)\"\"\"\n",
        "        pattern = r'[a-zA-Z]+\\d+|\\d+[a-zA-Z]+'\n",
        "        return len(re.findall(pattern, text))\n",
        "\n",
        "    def excessive_spacing(self, text):\n",
        "        \"\"\"Deteksi spasi berlebih antar karakter\"\"\"\n",
        "        pattern = r'(\\w\\s){3,}'\n",
        "        return len(re.findall(pattern, text))\n",
        "\n",
        "    def extract_features(self, texts):\n",
        "        \"\"\"Ekstraksi semua fitur\"\"\"\n",
        "        features = []\n",
        "        for text in texts:\n",
        "            features.append([\n",
        "                self.count_emoji(text),\n",
        "                self.capital_ratio(text),\n",
        "                self.has_numbers_in_word(text),\n",
        "                self.excessive_spacing(text)\n",
        "            ])\n",
        "        return np.array(features)\n",
        "\n",
        "\n",
        "class AdditionalFeaturesTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Transformer untuk fitur tambahan\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.feature_extractor = AdditionalFeatures()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return self.feature_extractor.extract_features(X)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# WORD2VEC & FASTTEXT TRANSFORMERS\n",
        "# ============================================================\n",
        "\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Transformer untuk Word2Vec embedding\"\"\"\n",
        "\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1, workers=4, sg=0):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.workers = workers\n",
        "        self.sg = sg  # 0=CBOW, 1=Skip-gram\n",
        "        self.model = None\n",
        "        self.preprocessor = TextPreprocessor()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Train Word2Vec model\"\"\"\n",
        "        # Tokenize sentences\n",
        "        sentences = [self.preprocessor.preprocess(text).split() for text in X]\n",
        "\n",
        "        # Train Word2Vec\n",
        "        self.model = Word2Vec(\n",
        "            sentences=sentences,\n",
        "            vector_size=self.vector_size,\n",
        "            window=self.window,\n",
        "            min_count=self.min_count,\n",
        "            workers=self.workers,\n",
        "            sg=self.sg\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform texts to averaged word vectors\"\"\"\n",
        "        vectors = []\n",
        "        for text in X:\n",
        "            text = self.preprocessor.preprocess(text)\n",
        "            words = text.split()\n",
        "\n",
        "            # Get vectors for words in vocabulary\n",
        "            word_vectors = [\n",
        "                self.model.wv[word] for word in words\n",
        "                if word in self.model.wv\n",
        "            ]\n",
        "\n",
        "            # Average word vectors\n",
        "            if word_vectors:\n",
        "                vectors.append(np.mean(word_vectors, axis=0))\n",
        "            else:\n",
        "                vectors.append(np.zeros(self.vector_size))\n",
        "\n",
        "        return np.array(vectors)\n",
        "\n",
        "\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Transformer untuk FastText embedding\"\"\"\n",
        "\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1, workers=4, sg=0):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.workers = workers\n",
        "        self.sg = sg  # 0=CBOW, 1=Skip-gram\n",
        "        self.model = None\n",
        "        self.preprocessor = TextPreprocessor()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Train FastText model\"\"\"\n",
        "        sentences = [self.preprocessor.preprocess(text).split() for text in X]\n",
        "\n",
        "        self.model = FastText(\n",
        "            sentences=sentences,\n",
        "            vector_size=self.vector_size,\n",
        "            window=self.window,\n",
        "            min_count=self.min_count,\n",
        "            workers=self.workers,\n",
        "            sg=self.sg\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform texts to averaged word vectors\"\"\"\n",
        "        vectors = []\n",
        "        for text in X:\n",
        "            text = self.preprocessor.preprocess(text)\n",
        "            words = text.split()\n",
        "\n",
        "            # FastText can handle OOV words\n",
        "            word_vectors = [self.model.wv[word] for word in words if words]\n",
        "\n",
        "            if word_vectors:\n",
        "                vectors.append(np.mean(word_vectors, axis=0))\n",
        "            else:\n",
        "                vectors.append(np.zeros(self.vector_size))\n",
        "\n",
        "        return np.array(vectors)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# MODEL CONFIGURATIONS\n",
        "# ============================================================\n",
        "\n",
        "def get_classifiers():\n",
        "    \"\"\"\n",
        "    Konfigurasi semua classifier yang tersedia\n",
        "    Returns dict: {nama_model: instance_model}\n",
        "    \"\"\"\n",
        "    classifiers = {\n",
        "        # Linear Models\n",
        "        'logistic_regression': LogisticRegression(\n",
        "            max_iter=1000,\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            solver='lbfgs'\n",
        "        ),\n",
        "\n",
        "        # Tree-based Models\n",
        "        'random_forest': RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=20,\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ),\n",
        "\n",
        "        'decision_tree': DecisionTreeClassifier(\n",
        "            max_depth=20,\n",
        "            class_weight='balanced',\n",
        "            random_state=42\n",
        "        ),\n",
        "\n",
        "        'gradient_boosting': GradientBoostingClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=5,\n",
        "            random_state=42\n",
        "        ),\n",
        "\n",
        "        # 'xgboost': XGBClassifier(\n",
        "        #     n_estimators=100,\n",
        "        #     max_depth=5,\n",
        "        #     learning_rate=0.1,\n",
        "        #     random_state=42,\n",
        "        #     eval_metric='logloss'\n",
        "        # ),\n",
        "\n",
        "        # 'lightgbm': LGBMClassifier(\n",
        "        #     n_estimators=100,\n",
        "        #     max_depth=5,\n",
        "        #     learning_rate=0.1,\n",
        "        #     random_state=42,\n",
        "        #     verbose=-1\n",
        "        # ),\n",
        "\n",
        "        # SVM\n",
        "        'svm_linear': SVC(\n",
        "            kernel='linear',\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            probability=True\n",
        "        ),\n",
        "\n",
        "        'svm_rbf': SVC(\n",
        "            kernel='rbf',\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            probability=True\n",
        "        ),\n",
        "\n",
        "        # Naive Bayes\n",
        "        'naive_bayes': MultinomialNB(alpha=1.0),\n",
        "\n",
        "        # KNN\n",
        "        'knn': KNeighborsClassifier(\n",
        "            n_neighbors=5,\n",
        "            weights='distance',\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    }\n",
        "\n",
        "    return classifiers\n",
        "\n",
        "\n",
        "def get_vectorizers():\n",
        "    \"\"\"\n",
        "    Konfigurasi semua vectorizer yang tersedia\n",
        "    Returns dict: {nama_vectorizer: config}\n",
        "    \"\"\"\n",
        "    preprocessor = TextPreprocessor()\n",
        "\n",
        "    vectorizers = {\n",
        "        # TF-IDF variants\n",
        "        'tfidf_char': TfidfVectorizer(\n",
        "            analyzer='char',\n",
        "            ngram_range=(2, 5),\n",
        "            max_features=5000,\n",
        "            min_df=2,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        'tfidf_word': TfidfVectorizer(\n",
        "            analyzer='word',\n",
        "            ngram_range=(1, 2),\n",
        "            max_features=5000,\n",
        "            min_df=2,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        'tfidf_char_wb': TfidfVectorizer(\n",
        "            analyzer='char_wb',\n",
        "            ngram_range=(2, 5),\n",
        "            max_features=5000,\n",
        "            min_df=2,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        # Count Vectorizer\n",
        "        'count_char': CountVectorizer(\n",
        "            analyzer='char',\n",
        "            ngram_range=(2, 5),\n",
        "            max_features=5000,\n",
        "            min_df=2,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        'count_word': CountVectorizer(\n",
        "            analyzer='word',\n",
        "            ngram_range=(1, 2),\n",
        "            max_features=5000,\n",
        "            min_df=2,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        # Hashing Vectorizer (memory efficient)\n",
        "        'hashing_char': HashingVectorizer(\n",
        "            analyzer='char',\n",
        "            ngram_range=(2, 5),\n",
        "            n_features=2**16,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        # Word2Vec\n",
        "        'word2vec_cbow': Word2VecTransformer(\n",
        "            vector_size=100,\n",
        "            window=5,\n",
        "            min_count=1,\n",
        "            sg=0  # CBOW\n",
        "        ),\n",
        "\n",
        "        'word2vec_skipgram': Word2VecTransformer(\n",
        "            vector_size=100,\n",
        "            window=5,\n",
        "            min_count=1,\n",
        "            sg=1  # Skip-gram\n",
        "        ),\n",
        "\n",
        "        # FastText\n",
        "        'fasttext_cbow': FastTextTransformer(\n",
        "            vector_size=100,\n",
        "            window=5,\n",
        "            min_count=1,\n",
        "            sg=0  # CBOW\n",
        "        ),\n",
        "\n",
        "        'fasttext_skipgram': FastTextTransformer(\n",
        "            vector_size=100,\n",
        "            window=5,\n",
        "            min_count=1,\n",
        "            sg=1  # Skip-gram\n",
        "        ),\n",
        "\n",
        "        # Hybrid combinations\n",
        "        'hybrid_word_char': FeatureUnion([\n",
        "            ('word_tfidf', TfidfVectorizer(\n",
        "                analyzer='word',\n",
        "                ngram_range=(1, 2),\n",
        "                max_features=3000,\n",
        "                preprocessor=preprocessor.preprocess\n",
        "            )),\n",
        "            ('char_tfidf', TfidfVectorizer(\n",
        "                analyzer='char',\n",
        "                ngram_range=(2, 5),\n",
        "                max_features=3000,\n",
        "                preprocessor=preprocessor.preprocess\n",
        "            ))\n",
        "        ]),\n",
        "\n",
        "        'hybrid_all_features': FeatureUnion([\n",
        "            ('word_tfidf', TfidfVectorizer(\n",
        "                analyzer='word',\n",
        "                ngram_range=(1, 2),\n",
        "                max_features=2000,\n",
        "                preprocessor=preprocessor.preprocess\n",
        "            )),\n",
        "            ('char_tfidf', TfidfVectorizer(\n",
        "                analyzer='char',\n",
        "                ngram_range=(2, 5),\n",
        "                max_features=2000,\n",
        "                preprocessor=preprocessor.preprocess\n",
        "            )),\n",
        "            ('additional', AdditionalFeaturesTransformer())\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    return vectorizers\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# PIPELINE BUILDER\n",
        "# ============================================================\n",
        "\n",
        "def create_custom_pipeline(vectorizer_name, classifier_name):\n",
        "    \"\"\"\n",
        "    Buat pipeline custom dengan kombinasi vectorizer dan classifier\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    vectorizer_name : str\n",
        "        Nama vectorizer dari get_vectorizers()\n",
        "    classifier_name : str\n",
        "        Nama classifier dari get_classifiers()\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Pipeline object\n",
        "    \"\"\"\n",
        "    vectorizers = get_vectorizers()\n",
        "    classifiers = get_classifiers()\n",
        "\n",
        "    if vectorizer_name not in vectorizers:\n",
        "        raise ValueError(f\"Vectorizer '{vectorizer_name}' tidak tersedia. \"\n",
        "                        f\"Pilihan: {list(vectorizers.keys())}\")\n",
        "\n",
        "    if classifier_name not in classifiers:\n",
        "        raise ValueError(f\"Classifier '{classifier_name}' tidak tersedia. \"\n",
        "                        f\"Pilihan: {list(classifiers.keys())}\")\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('vectorizer', vectorizers[vectorizer_name]),\n",
        "        ('classifier', classifiers[classifier_name])\n",
        "    ])\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING & EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "def train_and_evaluate(X, y, pipeline, pipeline_name):\n",
        "    \"\"\"Training dan evaluasi model\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EVALUASI: {pipeline_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    print(\"Training model...\")\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Prediction\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    # Evaluation\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred,\n",
        "                                target_names=['Non-Judi', 'Judi']))\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "    # Cross-validation\n",
        "    print(\"\\nPerforming cross-validation...\")\n",
        "    cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1')\n",
        "    print(f\"Cross-Validation F1-Score: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
        "\n",
        "    return pipeline, {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_score': f1,\n",
        "        'cv_f1_mean': cv_scores.mean(),\n",
        "        'cv_f1_std': cv_scores.std()\n",
        "    }\n",
        "\n",
        "\n",
        "def compare_multiple_models(X, y, vectorizer_configs, classifier_configs):\n",
        "    \"\"\"\n",
        "    Bandingkan multiple kombinasi vectorizer dan classifier\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : array-like\n",
        "        Text data\n",
        "    y : array-like\n",
        "        Labels\n",
        "    vectorizer_configs : list of str\n",
        "        List nama vectorizer yang ingin dicoba\n",
        "    classifier_configs : list of str\n",
        "        List nama classifier yang ingin dicoba\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    DataFrame dengan hasil perbandingan\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    total_experiments = len(vectorizer_configs) * len(classifier_configs)\n",
        "    experiment_num = 0\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"MEMULAI PERBANDINGAN {total_experiments} KOMBINASI MODEL\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    for vec_name in vectorizer_configs:\n",
        "        for clf_name in classifier_configs:\n",
        "            experiment_num += 1\n",
        "            print(f\"\\n[{experiment_num}/{total_experiments}] Testing: {vec_name} + {clf_name}\")\n",
        "\n",
        "            try:\n",
        "                # Create pipeline\n",
        "                pipeline = create_custom_pipeline(vec_name, clf_name)\n",
        "\n",
        "                # Train and evaluate\n",
        "                model, metrics = train_and_evaluate(\n",
        "                    X, y, pipeline,\n",
        "                    f\"{vec_name} + {clf_name}\"\n",
        "                )\n",
        "\n",
        "                # Store results\n",
        "                results.append({\n",
        "                    'vectorizer': vec_name,\n",
        "                    'classifier': clf_name,\n",
        "                    'accuracy': metrics['accuracy'],\n",
        "                    'f1_score': metrics['f1_score'],\n",
        "                    'cv_f1_mean': metrics['cv_f1_mean'],\n",
        "                    'cv_f1_std': metrics['cv_f1_std']\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error: {str(e)}\")\n",
        "                results.append({\n",
        "                    'vectorizer': vec_name,\n",
        "                    'classifier': clf_name,\n",
        "                    'accuracy': 0,\n",
        "                    'f1_score': 0,\n",
        "                    'cv_f1_mean': 0,\n",
        "                    'cv_f1_std': 0,\n",
        "                    'error': str(e)\n",
        "                })\n",
        "\n",
        "    # Create results dataframe\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df = results_df.sort_values('f1_score', ascending=False)\n",
        "\n",
        "    return results_df\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# QUICK PRESETS\n",
        "# ============================================================\n",
        "\n",
        "def create_pipeline_char_tfidf():\n",
        "    \"\"\"Pipeline 1: Character-Level TF-IDF (REKOMENDASI UTAMA)\"\"\"\n",
        "    return create_custom_pipeline('tfidf_char', 'logistic_regression')\n",
        "\n",
        "\n",
        "def create_pipeline_hybrid():\n",
        "    \"\"\"Pipeline 2: Hybrid (Word + Char)\"\"\"\n",
        "    return create_custom_pipeline('hybrid_word_char', 'logistic_regression')\n",
        "\n",
        "\n",
        "def create_pipeline_advanced():\n",
        "    \"\"\"Pipeline 3: Advanced (All Features)\"\"\"\n",
        "    return create_custom_pipeline('hybrid_all_features', 'random_forest')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S9K2Khix4eq",
        "outputId": "689a2957-a29d-417c-b632-cd2d72234a3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (11673,)\n",
            "Label distribution: [10522  1151]\n"
          ]
        }
      ],
      "source": [
        "X = df_all['comment'].values\n",
        "y = df_all['label'].values\n",
        "\n",
        "print(\"Dataset shape:\", X.shape)\n",
        "print(\"Label distribution:\", np.bincount(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPo_VrncyTM2",
        "outputId": "b92d0f6d-e105-4f3b-d7a2-3f359fe2c5e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "OPSI 1: QUICK PRESETS\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EVALUASI: Character TF-IDF\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      0.99      0.99      2105\n",
            "        Judi       0.94      0.93      0.94       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.97      0.96      0.96      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2091   14]\n",
            " [  15  215]]\n",
            "\n",
            "Accuracy: 0.9876\n",
            "F1-Score: 0.9368\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.8888 (+/- 0.1115)\n",
            "\n",
            "============================================================\n",
            "OPSI 2: CUSTOM PIPELINE\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EVALUASI: FastText CBOW + Random Forest\n",
            "============================================================\n",
            "Training model...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Contoh: FastText + Random Forest\u001b[39;00m\n\u001b[32m     19\u001b[39m custom_pipeline = create_custom_pipeline(\u001b[33m'\u001b[39m\u001b[33mfasttext_cbow\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrandom_forest\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m model_custom, metrics_custom = \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFastText CBOW + Random Forest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     23\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# OPSI 3: Compare Multiple Models\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 469\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m(X, y, pipeline, pipeline_name)\u001b[39m\n\u001b[32m    467\u001b[39m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[38;5;66;03m# Prediction\u001b[39;00m\n\u001b[32m    472\u001b[39m y_pred = pipeline.predict(X_test)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/sklearn/pipeline.py:655\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    649\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    650\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    651\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    652\u001b[39m     )\n\u001b[32m    654\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._check_method_params(method=\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, props=params)\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m Xt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps) - \u001b[32m1\u001b[39m)):\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/sklearn/pipeline.py:589\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, X, y, routed_params, raw_params)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[32m    583\u001b[39m step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    584\u001b[39m     step_idx=step_idx,\n\u001b[32m    585\u001b[39m     step_params=routed_params[name],\n\u001b[32m    586\u001b[39m     all_params=raw_params,\n\u001b[32m    587\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m X, fitted_transformer = \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPipeline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[32m    600\u001b[39m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[32m    601\u001b[39m \u001b[38;5;28mself\u001b[39m.steps[step_idx] = (name, fitted_transformer)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/joblib/memory.py:326\u001b[39m, in \u001b[36mNotMemorizedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/sklearn/pipeline.py:1540\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1539\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1541\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1542\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1543\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1544\u001b[39m         )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/sklearn/base.py:897\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    894\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, **fit_params).transform(X)\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    896\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 178\u001b[39m, in \u001b[36mFastTextTransformer.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Train FastText model\"\"\"\u001b[39;00m\n\u001b[32m    176\u001b[39m sentences = [\u001b[38;5;28mself\u001b[39m.preprocessor.preprocess(text).split() \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m X]\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mFastText\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m=\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43msg\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msg\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/gensim/models/fasttext.py:435\u001b[39m, in \u001b[36mFastText.__init__\u001b[39m\u001b[34m(self, sentences, corpus_file, sg, hs, vector_size, alpha, window, min_count, max_vocab_size, word_ngrams, sample, seed, workers, min_alpha, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, min_n, max_n, sorted_vocab, bucket, trim_rule, batch_words, callbacks, max_final_vocab, shrink_windows)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;28mself\u001b[39m.wv.vectors_vocab_lockf = ones(\u001b[32m1\u001b[39m, dtype=REAL)\n\u001b[32m    433\u001b[39m \u001b[38;5;28mself\u001b[39m.wv.vectors_ngrams_lockf = ones(\u001b[32m1\u001b[39m, dtype=REAL)\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFastText\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m=\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msg\u001b[49m\u001b[43m=\u001b[49m\u001b[43msg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_vocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_vocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_final_vocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_final_vocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorted_vocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43msorted_vocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnull_word\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnull_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mns_exponent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mns_exponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhashfxn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhashfxn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnegative\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbow_mean\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcbow_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshrink_windows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshrink_windows\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/gensim/models/word2vec.py:429\u001b[39m, in \u001b[36mWord2Vec.__init__\u001b[39m\u001b[34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m corpus_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    428\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_corpus_sanity(corpus_iterable=corpus_iterable, corpus_file=corpus_file, passes=(epochs + \u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    430\u001b[39m     \u001b[38;5;28mself\u001b[39m.train(\n\u001b[32m    431\u001b[39m         corpus_iterable=corpus_iterable, corpus_file=corpus_file, total_examples=\u001b[38;5;28mself\u001b[39m.corpus_count,\n\u001b[32m    432\u001b[39m         total_words=\u001b[38;5;28mself\u001b[39m.corpus_total_words, epochs=\u001b[38;5;28mself\u001b[39m.epochs, start_alpha=\u001b[38;5;28mself\u001b[39m.alpha,\n\u001b[32m    433\u001b[39m         end_alpha=\u001b[38;5;28mself\u001b[39m.min_alpha, compute_loss=\u001b[38;5;28mself\u001b[39m.compute_loss, callbacks=callbacks)\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/gensim/models/word2vec.py:497\u001b[39m, in \u001b[36mWord2Vec.build_vocab\u001b[39m\u001b[34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[39m\n\u001b[32m    495\u001b[39m report_values = \u001b[38;5;28mself\u001b[39m.prepare_vocab(update=update, keep_raw_vocab=keep_raw_vocab, trim_rule=trim_rule, **kwargs)\n\u001b[32m    496\u001b[39m report_values[\u001b[33m'\u001b[39m\u001b[33mmemory\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.estimate_memory(vocab_size=report_values[\u001b[33m'\u001b[39m\u001b[33mnum_retained_words\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;28mself\u001b[39m.add_lifecycle_event(\u001b[33m\"\u001b[39m\u001b[33mbuild_vocab\u001b[39m\u001b[33m\"\u001b[39m, update=update, trim_rule=\u001b[38;5;28mstr\u001b[39m(trim_rule))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/gensim/models/word2vec.py:853\u001b[39m, in \u001b[36mWord2Vec.prepare_weights\u001b[39m\u001b[34m(self, update)\u001b[39m\n\u001b[32m    851\u001b[39m \u001b[38;5;66;03m# set initial input/projection and hidden weights\u001b[39;00m\n\u001b[32m    852\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m update:\n\u001b[32m--> \u001b[39m\u001b[32m853\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minit_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    855\u001b[39m     \u001b[38;5;28mself\u001b[39m.update_weights()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/gensim/models/word2vec.py:864\u001b[39m, in \u001b[36mWord2Vec.init_weights\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    862\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\"\"\"\u001b[39;00m\n\u001b[32m    863\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mresetting layer weights\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m864\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hs:\n\u001b[32m    867\u001b[39m     \u001b[38;5;28mself\u001b[39m.syn1 = np.zeros((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.wv), \u001b[38;5;28mself\u001b[39m.layer1_size), dtype=REAL)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/gensim/models/fasttext.py:1156\u001b[39m, in \u001b[36mFastTextKeyedVectors.resize_vectors\u001b[39m\u001b[34m(self, seed)\u001b[39m\n\u001b[32m   1154\u001b[39m \u001b[38;5;28mself\u001b[39m.vectors_vocab = prep_vectors(vocab_shape, prior_vectors=\u001b[38;5;28mself\u001b[39m.vectors_vocab, seed=seed)\n\u001b[32m   1155\u001b[39m ngrams_shape = (\u001b[38;5;28mself\u001b[39m.bucket, \u001b[38;5;28mself\u001b[39m.vector_size)\n\u001b[32m-> \u001b[39m\u001b[32m1156\u001b[39m \u001b[38;5;28mself\u001b[39m.vectors_ngrams = \u001b[43mprep_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngrams_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior_vectors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvectors_ngrams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[38;5;28mself\u001b[39m.allocate_vecattrs()\n\u001b[32m   1159\u001b[39m \u001b[38;5;28mself\u001b[39m.norms = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/gensim/models/keyedvectors.py:2117\u001b[39m, in \u001b[36mprep_vectors\u001b[39m\u001b[34m(target_shape, prior_vectors, seed, dtype)\u001b[39m\n\u001b[32m   2115\u001b[39m target_count, vector_size = target_shape\n\u001b[32m   2116\u001b[39m rng = np.random.default_rng(seed=seed)  \u001b[38;5;66;03m# use new instance of numpy's recommended generator/algorithm\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2117\u001b[39m new_vectors = \u001b[43mrng\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [0.0, 1.0)\u001b[39;00m\n\u001b[32m   2118\u001b[39m new_vectors *= \u001b[32m2.0\u001b[39m  \u001b[38;5;66;03m# [0.0, 2.0)\u001b[39;00m\n\u001b[32m   2119\u001b[39m new_vectors -= \u001b[32m1.0\u001b[39m  \u001b[38;5;66;03m# [-1.0, 1.0)\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================\n",
        "# OPSI 1: Test Pipeline Quick Presets\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 1: QUICK PRESETS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "pipeline1 = create_pipeline_char_tfidf()\n",
        "model1, metrics1 = train_and_evaluate(X, y, pipeline1, \"Character TF-IDF\")\n",
        "\n",
        "# ============================================================\n",
        "# OPSI 2: Test Single Custom Pipeline\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 2: CUSTOM PIPELINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Contoh: FastText + Random Forest\n",
        "custom_pipeline = create_custom_pipeline('fasttext_cbow', 'random_forest')\n",
        "model_custom, metrics_custom = train_and_evaluate(\n",
        "    X, y, custom_pipeline,\n",
        "    \"FastText CBOW + Random Forest\"\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# OPSI 3: Compare Multiple Models\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 3: PERBANDINGAN MULTIPLE MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Pilih vectorizer dan classifier yang ingin dibandingkan\n",
        "vectorizers_to_test = [\n",
        "    'tfidf_char',\n",
        "    'tfidf_word',\n",
        "    'hybrid_word_char',\n",
        "    'fasttext_cbow',\n",
        "    'word2vec_cbow'\n",
        "]\n",
        "\n",
        "classifiers_to_test = [\n",
        "    'logistic_regression',\n",
        "    'random_forest',\n",
        "    # 'xgboost',\n",
        "    'svm_linear',\n",
        "    'svm_rbf',\n",
        "    'naive_bayes',\n",
        "    'knn',\n",
        "    # 'lightgbm',\n",
        "    'gradient_boosting'\n",
        "]\n",
        "\n",
        "# Jalankan perbandingan\n",
        "comparison_results = compare_multiple_models(\n",
        "    X, y,\n",
        "    vectorizers_to_test,\n",
        "    classifiers_to_test\n",
        ")\n",
        "\n",
        "    # Tampilkan hasil\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HASIL PERBANDINGAN MODEL (Sorted by F1-Score)\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_results.to_string(index=False))\n",
        "\n",
        "    # Top 5 models\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TOP 5 BEST MODELS\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_results.head().to_string(index=False))\n",
        "\n",
        "    # ============================================================\n",
        "    # OPSI 4: List Available Configurations\n",
        "    # ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"AVAILABLE CONFIGURATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nğŸ“Š Available Vectorizers:\")\n",
        "for i, vec in enumerate(get_vectorizers().keys(), 1):\n",
        "    print(f\"  {i}. {vec}\")\n",
        "\n",
        "print(\"\\nğŸ¤– Available Classifiers:\")\n",
        "for i, clf in enumerate(get_classifiers().keys(), 1):\n",
        "    print(f\"  {i}. {clf}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SlErfS1lTwjp"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "import re\n",
        "import unicodedata\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import warnings\n",
        "import time\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================\n",
        "# PREPROCESSING & FEATURE ENGINEERING\n",
        "# ============================================================\n",
        "\n",
        "class TextPreprocessor:\n",
        "    \"\"\"Preprocessing untuk menangani homoglyph dan variasi Unicode\"\"\"\n",
        "    def normalize_homoglyph(self, text):\n",
        "        \"\"\"Konversi homoglyph Unicode ke karakter normal\"\"\"\n",
        "        for homo, normal in HOMOGLYPH_MAP.items():\n",
        "            text = text.replace(homo, normal)\n",
        "        return text\n",
        "\n",
        "    def normalize_unicode(self, text):\n",
        "        \"\"\"Normalisasi Unicode menggunakan NFKD\"\"\"\n",
        "        return unicodedata.normalize('NFKD', text)\n",
        "\n",
        "    def remove_extra_spaces(self, text):\n",
        "        \"\"\"Hapus spasi berlebih\"\"\"\n",
        "        return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        \"\"\"Pipeline preprocessing lengkap\"\"\"\n",
        "        text = str(text).lower()\n",
        "        text = self.normalize_homoglyph(text)\n",
        "        text = self.normalize_unicode(text)\n",
        "        text = self.remove_extra_spaces(text)\n",
        "        return text\n",
        "\n",
        "\n",
        "class AdditionalFeatures:\n",
        "    \"\"\"Ekstraksi fitur tambahan untuk deteksi spam\"\"\"\n",
        "\n",
        "    def count_emoji(self, text):\n",
        "        \"\"\"Hitung jumlah emoji\"\"\"\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "            u\"\\U00002702-\\U000027B0\"\n",
        "            u\"\\U000024C2-\\U0001F251\"\n",
        "            \"]+\", flags=re.UNICODE)\n",
        "        return len(emoji_pattern.findall(text))\n",
        "\n",
        "    def capital_ratio(self, text):\n",
        "        \"\"\"Rasio huruf kapital\"\"\"\n",
        "        if len(text) == 0:\n",
        "            return 0\n",
        "        return sum(1 for c in text if c.isupper()) / len(text)\n",
        "\n",
        "    def has_numbers_in_word(self, text):\n",
        "        \"\"\"Deteksi angka dalam kata (SLOT88, PLUTO88)\"\"\"\n",
        "        pattern = r'[a-zA-Z]+\\d+|\\d+[a-zA-Z]+'\n",
        "        return len(re.findall(pattern, text))\n",
        "\n",
        "    def excessive_spacing(self, text):\n",
        "        \"\"\"Deteksi spasi berlebih antar karakter\"\"\"\n",
        "        pattern = r'(\\w\\s){3,}'\n",
        "        return len(re.findall(pattern, text))\n",
        "\n",
        "    def extract_features(self, texts):\n",
        "        \"\"\"Ekstraksi semua fitur\"\"\"\n",
        "        features = []\n",
        "        for text in texts:\n",
        "            features.append([\n",
        "                self.count_emoji(text),\n",
        "                self.capital_ratio(text),\n",
        "                self.has_numbers_in_word(text),\n",
        "                self.excessive_spacing(text)\n",
        "            ])\n",
        "        return np.array(features)\n",
        "\n",
        "\n",
        "class AdditionalFeaturesTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Transformer untuk fitur tambahan\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.feature_extractor = AdditionalFeatures()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return self.feature_extractor.extract_features(X)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# WORD2VEC & FASTTEXT TRANSFORMERS\n",
        "# ============================================================\n",
        "\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Transformer untuk Word2Vec embedding\"\"\"\n",
        "\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1, workers=4, sg=0):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.workers = workers\n",
        "        self.sg = sg  # 0=CBOW, 1=Skip-gram\n",
        "        self.model = None\n",
        "        self.preprocessor = TextPreprocessor()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Train Word2Vec model\"\"\"\n",
        "        # Tokenize sentences\n",
        "        sentences = [self.preprocessor.preprocess(text).split() for text in X]\n",
        "\n",
        "        # Train Word2Vec\n",
        "        self.model = Word2Vec(\n",
        "            sentences=sentences,\n",
        "            vector_size=self.vector_size,\n",
        "            window=self.window,\n",
        "            min_count=self.min_count,\n",
        "            workers=self.workers,\n",
        "            sg=self.sg\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform texts to averaged word vectors\"\"\"\n",
        "        vectors = []\n",
        "        for text in X:\n",
        "            text = self.preprocessor.preprocess(text)\n",
        "            words = text.split()\n",
        "\n",
        "            # Get vectors for words in vocabulary\n",
        "            word_vectors = [\n",
        "                self.model.wv[word] for word in words\n",
        "                if word in self.model.wv\n",
        "            ]\n",
        "\n",
        "            # Average word vectors\n",
        "            if word_vectors:\n",
        "                vectors.append(np.mean(word_vectors, axis=0))\n",
        "            else:\n",
        "                vectors.append(np.zeros(self.vector_size))\n",
        "\n",
        "        return np.array(vectors)\n",
        "\n",
        "\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Transformer untuk FastText embedding\"\"\"\n",
        "\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1, workers=4, sg=0):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.workers = workers\n",
        "        self.sg = sg  # 0=CBOW, 1=Skip-gram\n",
        "        self.model = None\n",
        "        self.preprocessor = TextPreprocessor()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Train FastText model\"\"\"\n",
        "        sentences = [self.preprocessor.preprocess(text).split() for text in X]\n",
        "\n",
        "        self.model = FastText(\n",
        "            sentences=sentences,\n",
        "            vector_size=self.vector_size,\n",
        "            window=self.window,\n",
        "            min_count=self.min_count,\n",
        "            workers=self.workers,\n",
        "            sg=self.sg\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform texts to averaged word vectors\"\"\"\n",
        "        vectors = []\n",
        "        for text in X:\n",
        "            text = self.preprocessor.preprocess(text)\n",
        "            words = text.split()\n",
        "\n",
        "            # FastText can handle OOV words\n",
        "            word_vectors = [self.model.wv[word] for word in words if words]\n",
        "\n",
        "            if word_vectors:\n",
        "                vectors.append(np.mean(word_vectors, axis=0))\n",
        "            else:\n",
        "                vectors.append(np.zeros(self.vector_size))\n",
        "\n",
        "        return np.array(vectors)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# MODEL CONFIGURATIONS\n",
        "# ============================================================\n",
        "\n",
        "def get_classifiers():\n",
        "    \"\"\"\n",
        "    Konfigurasi semua classifier yang tersedia\n",
        "    Returns dict: {nama_model: instance_model}\n",
        "    \"\"\"\n",
        "    classifiers = {\n",
        "        # Linear Models\n",
        "        'logistic_regression': LogisticRegression(\n",
        "            max_iter=1000,\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            solver='lbfgs'\n",
        "        ),\n",
        "\n",
        "        # Tree-based Models\n",
        "        'random_forest': RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=20,\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ),\n",
        "\n",
        "        'decision_tree': DecisionTreeClassifier(\n",
        "            max_depth=20,\n",
        "            class_weight='balanced',\n",
        "            random_state=42\n",
        "        ),\n",
        "\n",
        "        'gradient_boosting': GradientBoostingClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=5,\n",
        "            random_state=42\n",
        "        ),\n",
        "\n",
        "        'xgboost': XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=5,\n",
        "            learning_rate=0.1,\n",
        "            random_state=42,\n",
        "            eval_metric='logloss'\n",
        "        ),\n",
        "\n",
        "        'lightgbm': LGBMClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=5,\n",
        "            learning_rate=0.1,\n",
        "            random_state=42,\n",
        "            verbose=-1\n",
        "        ),\n",
        "\n",
        "        # SVM\n",
        "        'svm_linear': SVC(\n",
        "            kernel='linear',\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            probability=True\n",
        "        ),\n",
        "\n",
        "        'svm_rbf': SVC(\n",
        "            kernel='rbf',\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            probability=True\n",
        "        ),\n",
        "\n",
        "        # Naive Bayes\n",
        "        'naive_bayes': MultinomialNB(alpha=1.0),\n",
        "\n",
        "        # KNN\n",
        "        'knn': KNeighborsClassifier(\n",
        "            n_neighbors=5,\n",
        "            weights='distance',\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    }\n",
        "\n",
        "    return classifiers\n",
        "\n",
        "\n",
        "def get_vectorizers():\n",
        "    \"\"\"\n",
        "    Konfigurasi semua vectorizer yang tersedia\n",
        "    Returns dict: {nama_vectorizer: config}\n",
        "    \"\"\"\n",
        "    preprocessor = TextPreprocessor()\n",
        "\n",
        "    vectorizers = {\n",
        "        # TF-IDF variants\n",
        "        'tfidf_char': TfidfVectorizer(\n",
        "            analyzer='char',\n",
        "            ngram_range=(2, 5),\n",
        "            max_features=5000,\n",
        "            min_df=2,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        'tfidf_word': TfidfVectorizer(\n",
        "            analyzer='word',\n",
        "            ngram_range=(1, 2),\n",
        "            max_features=5000,\n",
        "            min_df=2,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        'tfidf_char_wb': TfidfVectorizer(\n",
        "            analyzer='char_wb',\n",
        "            ngram_range=(2, 5),\n",
        "            max_features=5000,\n",
        "            min_df=2,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        # Count Vectorizer\n",
        "        'count_char': CountVectorizer(\n",
        "            analyzer='char',\n",
        "            ngram_range=(2, 5),\n",
        "            max_features=5000,\n",
        "            min_df=2,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        'count_word': CountVectorizer(\n",
        "            analyzer='word',\n",
        "            ngram_range=(1, 2),\n",
        "            max_features=5000,\n",
        "            min_df=2,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        # Hashing Vectorizer (memory efficient)\n",
        "        'hashing_char': HashingVectorizer(\n",
        "            analyzer='char',\n",
        "            ngram_range=(2, 5),\n",
        "            n_features=2**16,\n",
        "            preprocessor=preprocessor.preprocess\n",
        "        ),\n",
        "\n",
        "        # Word2Vec\n",
        "        'word2vec_cbow': Word2VecTransformer(\n",
        "            vector_size=100,\n",
        "            window=5,\n",
        "            min_count=1,\n",
        "            sg=0  # CBOW\n",
        "        ),\n",
        "\n",
        "        'word2vec_skipgram': Word2VecTransformer(\n",
        "            vector_size=100,\n",
        "            window=5,\n",
        "            min_count=1,\n",
        "            sg=1  # Skip-gram\n",
        "        ),\n",
        "\n",
        "        # FastText\n",
        "        'fasttext_cbow': FastTextTransformer(\n",
        "            vector_size=100,\n",
        "            window=5,\n",
        "            min_count=1,\n",
        "            sg=0  # CBOW\n",
        "        ),\n",
        "\n",
        "        'fasttext_skipgram': FastTextTransformer(\n",
        "            vector_size=100,\n",
        "            window=5,\n",
        "            min_count=1,\n",
        "            sg=1  # Skip-gram\n",
        "        ),\n",
        "\n",
        "        # Hybrid combinations\n",
        "        'hybrid_word_char': FeatureUnion([\n",
        "            ('word_tfidf', TfidfVectorizer(\n",
        "                analyzer='word',\n",
        "                ngram_range=(1, 2),\n",
        "                max_features=3000,\n",
        "                preprocessor=preprocessor.preprocess\n",
        "            )),\n",
        "            ('char_tfidf', TfidfVectorizer(\n",
        "                analyzer='char',\n",
        "                ngram_range=(2, 5),\n",
        "                max_features=3000,\n",
        "                preprocessor=preprocessor.preprocess\n",
        "            ))\n",
        "        ]),\n",
        "\n",
        "        'hybrid_all_features': FeatureUnion([\n",
        "            ('word_tfidf', TfidfVectorizer(\n",
        "                analyzer='word',\n",
        "                ngram_range=(1, 2),\n",
        "                max_features=2000,\n",
        "                preprocessor=preprocessor.preprocess\n",
        "            )),\n",
        "            ('char_tfidf', TfidfVectorizer(\n",
        "                analyzer='char',\n",
        "                ngram_range=(2, 5),\n",
        "                max_features=2000,\n",
        "                preprocessor=preprocessor.preprocess\n",
        "            )),\n",
        "            ('additional', AdditionalFeaturesTransformer())\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    return vectorizers\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# PIPELINE BUILDER\n",
        "# ============================================================\n",
        "\n",
        "def create_custom_pipeline(vectorizer_name, classifier_name):\n",
        "    \"\"\"\n",
        "    Buat pipeline custom dengan kombinasi vectorizer dan classifier\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    vectorizer_name : str\n",
        "        Nama vectorizer dari get_vectorizers()\n",
        "    classifier_name : str\n",
        "        Nama classifier dari get_classifiers()\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Pipeline object\n",
        "    \"\"\"\n",
        "    vectorizers = get_vectorizers()\n",
        "    classifiers = get_classifiers()\n",
        "\n",
        "    if vectorizer_name not in vectorizers:\n",
        "        raise ValueError(f\"Vectorizer '{vectorizer_name}' tidak tersedia. \"\n",
        "                        f\"Pilihan: {list(vectorizers.keys())}\")\n",
        "\n",
        "    if classifier_name not in classifiers:\n",
        "        raise ValueError(f\"Classifier '{classifier_name}' tidak tersedia. \"\n",
        "                        f\"Pilihan: {list(classifiers.keys())}\")\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('vectorizer', vectorizers[vectorizer_name]),\n",
        "        ('classifier', classifiers[classifier_name])\n",
        "    ])\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TIMING UTILITIES\n",
        "# ============================================================\n",
        "\n",
        "def measure_single_prediction_time(pipeline, sample_text, num_iterations=100):\n",
        "    \"\"\"\n",
        "    Ukur waktu rata-rata untuk memprediksi satu teks\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    pipeline : fitted Pipeline\n",
        "        Model yang sudah ditraining\n",
        "    sample_text : str\n",
        "        Contoh teks untuk prediksi\n",
        "    num_iterations : int\n",
        "        Jumlah iterasi untuk mengambil rata-rata\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : {\n",
        "        'avg_time_ms': waktu rata-rata dalam milidetik,\n",
        "        'avg_time_sec': waktu rata-rata dalam detik,\n",
        "        'total_time_sec': total waktu untuk semua iterasi\n",
        "    }\n",
        "    \"\"\"\n",
        "    times = []\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        start_time = time.time()\n",
        "        pipeline.predict([sample_text])\n",
        "        end_time = time.time()\n",
        "        times.append(end_time - start_time)\n",
        "\n",
        "    avg_time = np.mean(times)\n",
        "\n",
        "    return {\n",
        "        'avg_time_ms': avg_time * 1000,\n",
        "        'avg_time_sec': avg_time,\n",
        "        'total_time_sec': sum(times),\n",
        "        'min_time_ms': min(times) * 1000,\n",
        "        'max_time_ms': max(times) * 1000\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING & EVALUATION WITH TIMING\n",
        "# ============================================================\n",
        "\n",
        "def train_and_evaluate(X, y, pipeline, pipeline_name):\n",
        "    \"\"\"Training dan evaluasi model dengan pengukuran waktu\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EVALUASI: {pipeline_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Training with timing\n",
        "    print(\"Training model...\")\n",
        "    train_start = time.time()\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    train_time = time.time() - train_start\n",
        "    print(f\"â±ï¸  Training Time: {train_time:.4f} seconds\")\n",
        "\n",
        "    # Prediction with timing\n",
        "    print(\"\\nTesting prediction speed...\")\n",
        "    pred_start = time.time()\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    pred_time = time.time() - pred_start\n",
        "    avg_pred_time_per_sample = (pred_time / len(X_test)) * 1000  # in milliseconds\n",
        "    print(f\"â±ï¸  Batch Prediction Time: {pred_time:.4f} seconds\")\n",
        "    print(f\"â±ï¸  Average Time per Sample: {avg_pred_time_per_sample:.4f} ms\")\n",
        "\n",
        "    # Single prediction timing (more accurate for real-time use)\n",
        "    print(\"\\nMeasuring single prediction latency (100 iterations)...\")\n",
        "    if len(X_test) > 0:\n",
        "        single_pred_timing = measure_single_prediction_time(\n",
        "            pipeline, X_test.iloc[0] if hasattr(X_test, 'iloc') else X_test[0],\n",
        "            num_iterations=100\n",
        "        )\n",
        "        print(f\"â±ï¸  Single Prediction Time (avg): {single_pred_timing['avg_time_ms']:.4f} ms\")\n",
        "        print(f\"â±ï¸  Single Prediction Time (min): {single_pred_timing['min_time_ms']:.4f} ms\")\n",
        "        print(f\"â±ï¸  Single Prediction Time (max): {single_pred_timing['max_time_ms']:.4f} ms\")\n",
        "    else:\n",
        "        single_pred_timing = {'avg_time_ms': 0}\n",
        "\n",
        "    # Evaluation\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred,\n",
        "                                target_names=['Non-Judi', 'Judi']))\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "    # Cross-validation with timing\n",
        "    print(\"\\nPerforming cross-validation...\")\n",
        "    cv_start = time.time()\n",
        "    cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1')\n",
        "    cv_time = time.time() - cv_start\n",
        "    print(f\"â±ï¸  Cross-Validation Time: {cv_time:.4f} seconds ({cv_time/60:.2f} minutes)\")\n",
        "    print(f\"Cross-Validation F1-Score: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
        "\n",
        "    return pipeline, {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'cv_f1_mean': cv_scores.mean(),\n",
        "        'cv_f1_std': cv_scores.std(),\n",
        "        'train_time_sec': train_time,\n",
        "        'batch_pred_time_sec': pred_time,\n",
        "        'avg_pred_time_ms': avg_pred_time_per_sample,\n",
        "        'single_pred_time_ms': single_pred_timing['avg_time_ms'],\n",
        "        'single_pred_min_ms': single_pred_timing.get('min_time_ms', 0),\n",
        "        'single_pred_max_ms': single_pred_timing.get('max_time_ms', 0),\n",
        "        'cv_time_sec': cv_time,\n",
        "        'cv_time_min': cv_time / 60\n",
        "    }\n",
        "\n",
        "\n",
        "def compare_multiple_models(X, y, vectorizer_configs, classifier_configs):\n",
        "    \"\"\"\n",
        "    Bandingkan multiple kombinasi vectorizer dan classifier dengan timing\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : array-like\n",
        "        Text data\n",
        "    y : array-like\n",
        "        Labels\n",
        "    vectorizer_configs : list of str\n",
        "        List nama vectorizer yang ingin dicoba\n",
        "    classifier_configs : list of str\n",
        "        List nama classifier yang ingin dicoba\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    DataFrame dengan hasil perbandingan termasuk timing\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    total_experiments = len(vectorizer_configs) * len(classifier_configs)\n",
        "    experiment_num = 0\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"MEMULAI PERBANDINGAN {total_experiments} KOMBINASI MODEL\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    for vec_name in vectorizer_configs:\n",
        "        for clf_name in classifier_configs:\n",
        "            experiment_num += 1\n",
        "            print(f\"\\n[{experiment_num}/{total_experiments}] Testing: {vec_name} + {clf_name}\")\n",
        "\n",
        "            try:\n",
        "                # Create pipeline\n",
        "                pipeline = create_custom_pipeline(vec_name, clf_name)\n",
        "\n",
        "                # Train and evaluate\n",
        "                model, metrics = train_and_evaluate(\n",
        "                    X, y, pipeline,\n",
        "                    f\"{vec_name} + {clf_name}\"\n",
        "                )\n",
        "\n",
        "                # Store results\n",
        "                results.append({\n",
        "                    'vectorizer': vec_name,\n",
        "                    'classifier': clf_name,\n",
        "                    'accuracy': metrics['accuracy'],\n",
        "                    'precision': metrics['precision'],\n",
        "                    'recall': metrics['recall'],\n",
        "                    'f1_score': metrics['f1_score'],\n",
        "                    'cv_f1_mean': metrics['cv_f1_mean'],\n",
        "                    'cv_f1_std': metrics['cv_f1_std'],\n",
        "                    'train_time_sec': metrics['train_time_sec'],\n",
        "                    'single_pred_ms': metrics['single_pred_time_ms'],\n",
        "                    'single_pred_min_ms': metrics['single_pred_min_ms'],\n",
        "                    'single_pred_max_ms': metrics['single_pred_max_ms'],\n",
        "                    'cv_time_min': metrics['cv_time_min']\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error: {str(e)}\")\n",
        "                results.append({\n",
        "                    'vectorizer': vec_name,\n",
        "                    'classifier': clf_name,\n",
        "                    'accuracy': 0,\n",
        "                    'precision': 0,\n",
        "                    'recall': 0,\n",
        "                    'f1_score': 0,\n",
        "                    'cv_f1_mean': 0,\n",
        "                    'cv_f1_std': 0,\n",
        "                    'train_time_sec': 0,\n",
        "                    'single_pred_ms': 0,\n",
        "                    'single_pred_min_ms': 0,\n",
        "                    'single_pred_max_ms': 0,\n",
        "                    'cv_time_min': 0,\n",
        "                    'error': str(e)\n",
        "                })\n",
        "\n",
        "    # Create results dataframe\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df = results_df.sort_values('f1_score', ascending=False)\n",
        "\n",
        "    # Display summary\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"RANKING BERDASARKAN F1-SCORE\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(results_df[['vectorizer', 'classifier', 'precision', 'recall', 'f1_score', 'single_pred_ms']].head(10))\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"RANKING BERDASARKAN KECEPATAN PREDIKSI (FASTEST)\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(results_df.sort_values('single_pred_ms')[['vectorizer', 'classifier', 'precision', 'recall', 'f1_score', 'single_pred_ms']].head(10))\n",
        "\n",
        "    return results_df\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# QUICK PRESETS\n",
        "# ============================================================\n",
        "\n",
        "def create_pipeline_char_tfidf():\n",
        "    \"\"\"Pipeline 1: Character-Level TF-IDF (REKOMENDASI UTAMA)\"\"\"\n",
        "    return create_custom_pipeline('tfidf_char', 'logistic_regression')\n",
        "\n",
        "\n",
        "def create_pipeline_hybrid():\n",
        "    \"\"\"Pipeline 2: Hybrid (Word + Char)\"\"\"\n",
        "    return create_custom_pipeline('hybrid_word_char', 'logistic_regression')\n",
        "\n",
        "\n",
        "def create_pipeline_advanced():\n",
        "    \"\"\"Pipeline 3: Advanced (All Features)\"\"\"\n",
        "    return create_custom_pipeline('hybrid_all_features', 'random_forest')\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# VISUALIZATION HELPER\n",
        "# ============================================================\n",
        "\n",
        "def create_comparison_table(results_df):\n",
        "    \"\"\"\n",
        "    Buat tabel perbandingan yang lebih mudah dibaca\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    results_df : DataFrame\n",
        "        Hasil dari compare_multiple_models()\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    DataFrame dengan format yang lebih readable\n",
        "    \"\"\"\n",
        "    comparison_df = results_df.copy()\n",
        "\n",
        "    # Format kolom\n",
        "    comparison_df['model_name'] = comparison_df['vectorizer'] + ' + ' + comparison_df['classifier']\n",
        "    comparison_df['f1_score_pct'] = (comparison_df['f1_score'] * 100).round(2)\n",
        "    comparison_df['accuracy_pct'] = (comparison_df['accuracy'] * 100).round(2)\n",
        "    comparison_df['precision_pct'] = (comparison_df['precision'] * 100).round(2)\n",
        "    comparison_df['recall_pct'] = (comparison_df['recall'] * 100).round(2)\n",
        "    comparison_df['cv_f1_pct'] = (comparison_df['cv_f1_mean'] * 100).round(2)\n",
        "    comparison_df['pred_speed'] = comparison_df['single_pred_ms'].round(4)\n",
        "    comparison_df['cv_time'] = comparison_df['cv_time_min'].round(2)\n",
        "\n",
        "    # Select dan rename kolom\n",
        "    final_df = comparison_df[[\n",
        "        'model_name',\n",
        "        'accuracy_pct',\n",
        "        'precision_pct',\n",
        "        'recall_pct',\n",
        "        'f1_score_pct',\n",
        "        'cv_f1_pct',\n",
        "        'pred_speed',\n",
        "        'train_time_sec',\n",
        "        'cv_time'\n",
        "    ]].copy()\n",
        "\n",
        "    final_df.columns = [\n",
        "        'Model',\n",
        "        'Accuracy (%)',\n",
        "        'Precision (%)',\n",
        "        'Recall (%)',\n",
        "        'F1-Score (%)',\n",
        "        'CV F1-Score (%)',\n",
        "        'Pred Time (ms)',\n",
        "        'Train Time (s)',\n",
        "        'CV Time (min)'\n",
        "    ]\n",
        "\n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQCEWmzhScev",
        "outputId": "0a2e5e76-fb98-4dd9-c870-401bba79ea13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "OPSI 1: QUICK PRESETS\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EVALUASI: Character TF-IDF\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 3.2607 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 0.6717 seconds\n",
            "â±ï¸  Average Time per Sample: 0.2877 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 1.4892 ms\n",
            "â±ï¸  Single Prediction Time (min): 1.2794 ms\n",
            "â±ï¸  Single Prediction Time (max): 3.4258 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      0.99      0.99      2105\n",
            "        Judi       0.94      0.93      0.94       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.97      0.96      0.96      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2091   14]\n",
            " [  15  215]]\n",
            "\n",
            "Accuracy: 0.9876\n",
            "Precision: 0.9389\n",
            "Recall: 0.9348\n",
            "F1-Score: 0.9368\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 17.2031 seconds (0.29 minutes)\n",
            "Cross-Validation F1-Score: 0.8888 (+/- 0.1115)\n",
            "\n",
            "============================================================\n",
            "OPSI 2: CUSTOM PIPELINE\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EVALUASI: FastText CBOW + Random Forest\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 16.1586 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 0.3077 seconds\n",
            "â±ï¸  Average Time per Sample: 0.1318 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 33.9015 ms\n",
            "â±ï¸  Single Prediction Time (min): 23.9086 ms\n",
            "â±ï¸  Single Prediction Time (max): 36.7796 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.96      0.98      0.97      2105\n",
            "        Judi       0.79      0.60      0.68       230\n",
            "\n",
            "    accuracy                           0.94      2335\n",
            "   macro avg       0.87      0.79      0.82      2335\n",
            "weighted avg       0.94      0.94      0.94      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2069   36]\n",
            " [  93  137]]\n",
            "\n",
            "Accuracy: 0.9448\n",
            "Precision: 0.7919\n",
            "Recall: 0.5957\n",
            "F1-Score: 0.6799\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 82.1778 seconds (1.37 minutes)\n",
            "Cross-Validation F1-Score: 0.6432 (+/- 0.1564)\n",
            "\n",
            "============================================================\n",
            "OPSI 3: PERBANDINGAN MULTIPLE MODELS\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "MEMULAI PERBANDINGAN 18 KOMBINASI MODEL\n",
            "============================================================\n",
            "\n",
            "\n",
            "[1/18] Testing: tfidf_char + logistic_regression\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + logistic_regression\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 3.8896 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 0.3806 seconds\n",
            "â±ï¸  Average Time per Sample: 0.1630 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 0.9771 ms\n",
            "â±ï¸  Single Prediction Time (min): 0.8657 ms\n",
            "â±ï¸  Single Prediction Time (max): 2.0859 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      0.99      0.99      2105\n",
            "        Judi       0.94      0.93      0.94       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.97      0.96      0.96      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2091   14]\n",
            " [  15  215]]\n",
            "\n",
            "Accuracy: 0.9876\n",
            "Precision: 0.9389\n",
            "Recall: 0.9348\n",
            "F1-Score: 0.9368\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 16.8520 seconds (0.28 minutes)\n",
            "Cross-Validation F1-Score: 0.8888 (+/- 0.1115)\n",
            "\n",
            "[2/18] Testing: tfidf_char + random_forest\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + random_forest\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 6.1702 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 0.8099 seconds\n",
            "â±ï¸  Average Time per Sample: 0.3468 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 34.4557 ms\n",
            "â±ï¸  Single Prediction Time (min): 24.8430 ms\n",
            "â±ï¸  Single Prediction Time (max): 51.8684 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.98      1.00      0.99      2105\n",
            "        Judi       0.97      0.84      0.90       230\n",
            "\n",
            "    accuracy                           0.98      2335\n",
            "   macro avg       0.98      0.92      0.94      2335\n",
            "weighted avg       0.98      0.98      0.98      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2099    6]\n",
            " [  37  193]]\n",
            "\n",
            "Accuracy: 0.9816\n",
            "Precision: 0.9698\n",
            "Recall: 0.8391\n",
            "F1-Score: 0.8998\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 34.6217 seconds (0.58 minutes)\n",
            "Cross-Validation F1-Score: 0.8055 (+/- 0.1961)\n",
            "\n",
            "[3/18] Testing: tfidf_char + xgboost\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + xgboost\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 31.9616 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 0.4127 seconds\n",
            "â±ï¸  Average Time per Sample: 0.1767 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 2.1019 ms\n",
            "â±ï¸  Single Prediction Time (min): 1.4861 ms\n",
            "â±ï¸  Single Prediction Time (max): 7.1149 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.99      0.89      0.94       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.99      0.95      0.97      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2103    2]\n",
            " [  25  205]]\n",
            "\n",
            "Accuracy: 0.9884\n",
            "Precision: 0.9903\n",
            "Recall: 0.8913\n",
            "F1-Score: 0.9382\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 144.3294 seconds (2.41 minutes)\n",
            "Cross-Validation F1-Score: 0.8178 (+/- 0.2613)\n",
            "\n",
            "[4/18] Testing: tfidf_char + svm_linear\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + svm_linear\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 68.7856 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 2.6458 seconds\n",
            "â±ï¸  Average Time per Sample: 1.1331 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 2.8346 ms\n",
            "â±ï¸  Single Prediction Time (min): 2.4970 ms\n",
            "â±ï¸  Single Prediction Time (max): 4.7123 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       1.00      0.99      0.99      2105\n",
            "        Judi       0.94      0.96      0.95       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.97      0.98      0.97      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2092   13]\n",
            " [  10  220]]\n",
            "\n",
            "Accuracy: 0.9901\n",
            "Precision: 0.9442\n",
            "Recall: 0.9565\n",
            "F1-Score: 0.9503\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 355.1844 seconds (5.92 minutes)\n",
            "Cross-Validation F1-Score: 0.8832 (+/- 0.1428)\n",
            "\n",
            "[5/18] Testing: tfidf_char + svm_rbf\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + svm_rbf\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 130.2934 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 4.8350 seconds\n",
            "â±ï¸  Average Time per Sample: 2.0707 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 4.9756 ms\n",
            "â±ï¸  Single Prediction Time (min): 4.5521 ms\n",
            "â±ï¸  Single Prediction Time (max): 5.9624 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.99      0.90      0.95       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.99      0.95      0.97      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2103    2]\n",
            " [  22  208]]\n",
            "\n",
            "Accuracy: 0.9897\n",
            "Precision: 0.9905\n",
            "Recall: 0.9043\n",
            "F1-Score: 0.9455\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 674.3768 seconds (11.24 minutes)\n",
            "Cross-Validation F1-Score: 0.8443 (+/- 0.2100)\n",
            "\n",
            "[6/18] Testing: tfidf_char + naive_bayes\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + naive_bayes\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 2.6327 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 0.3909 seconds\n",
            "â±ï¸  Average Time per Sample: 0.1674 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 0.9403 ms\n",
            "â±ï¸  Single Prediction Time (min): 0.8836 ms\n",
            "â±ï¸  Single Prediction Time (max): 1.7738 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.98      1.00      0.99      2105\n",
            "        Judi       0.97      0.79      0.87       230\n",
            "\n",
            "    accuracy                           0.98      2335\n",
            "   macro avg       0.98      0.89      0.93      2335\n",
            "weighted avg       0.98      0.98      0.98      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2100    5]\n",
            " [  49  181]]\n",
            "\n",
            "Accuracy: 0.9769\n",
            "Precision: 0.9731\n",
            "Recall: 0.7870\n",
            "F1-Score: 0.8702\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 16.1022 seconds (0.27 minutes)\n",
            "Cross-Validation F1-Score: 0.7916 (+/- 0.1420)\n",
            "\n",
            "[7/18] Testing: tfidf_char + knn\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + knn\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 2.5721 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 2.9751 seconds\n",
            "â±ï¸  Average Time per Sample: 1.2742 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 39.3729 ms\n",
            "â±ï¸  Single Prediction Time (min): 33.7527 ms\n",
            "â±ï¸  Single Prediction Time (max): 89.7937 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.95      1.00      0.97      2105\n",
            "        Judi       0.99      0.50      0.66       230\n",
            "\n",
            "    accuracy                           0.95      2335\n",
            "   macro avg       0.97      0.75      0.82      2335\n",
            "weighted avg       0.95      0.95      0.94      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2104    1]\n",
            " [ 115  115]]\n",
            "\n",
            "Accuracy: 0.9503\n",
            "Precision: 0.9914\n",
            "Recall: 0.5000\n",
            "F1-Score: 0.6647\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 26.9645 seconds (0.45 minutes)\n",
            "Cross-Validation F1-Score: 0.5566 (+/- 0.2031)\n",
            "\n",
            "[8/18] Testing: tfidf_char + lightgbm\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + lightgbm\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 10.0404 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 0.4165 seconds\n",
            "â±ï¸  Average Time per Sample: 0.1784 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 13.0914 ms\n",
            "â±ï¸  Single Prediction Time (min): 8.0714 ms\n",
            "â±ï¸  Single Prediction Time (max): 235.3642 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.99      0.91      0.95       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.99      0.95      0.97      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2103    2]\n",
            " [  21  209]]\n",
            "\n",
            "Accuracy: 0.9901\n",
            "Precision: 0.9905\n",
            "Recall: 0.9087\n",
            "F1-Score: 0.9478\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 51.8752 seconds (0.86 minutes)\n",
            "Cross-Validation F1-Score: 0.8209 (+/- 0.2623)\n",
            "\n",
            "[9/18] Testing: tfidf_char + gradient_boosting\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + gradient_boosting\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 109.5585 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 0.4145 seconds\n",
            "â±ï¸  Average Time per Sample: 0.1775 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 1.3106 ms\n",
            "â±ï¸  Single Prediction Time (min): 1.1995 ms\n",
            "â±ï¸  Single Prediction Time (max): 2.5303 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.98      0.89      0.93       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.98      0.94      0.96      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2101    4]\n",
            " [  26  204]]\n",
            "\n",
            "Accuracy: 0.9872\n",
            "Precision: 0.9808\n",
            "Recall: 0.8870\n",
            "F1-Score: 0.9315\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 552.7661 seconds (9.21 minutes)\n",
            "Cross-Validation F1-Score: 0.8167 (+/- 0.2579)\n",
            "\n",
            "[10/18] Testing: hybrid_word_char + logistic_regression\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + logistic_regression\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 4.3088 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 0.4792 seconds\n",
            "â±ï¸  Average Time per Sample: 0.2052 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 2.1641 ms\n",
            "â±ï¸  Single Prediction Time (min): 1.9088 ms\n",
            "â±ï¸  Single Prediction Time (max): 3.4857 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.96      0.93      0.95       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.98      0.97      0.97      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2097    8]\n",
            " [  15  215]]\n",
            "\n",
            "Accuracy: 0.9901\n",
            "Precision: 0.9641\n",
            "Recall: 0.9348\n",
            "F1-Score: 0.9492\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 20.9685 seconds (0.35 minutes)\n",
            "Cross-Validation F1-Score: 0.8913 (+/- 0.0983)\n",
            "\n",
            "[11/18] Testing: hybrid_word_char + random_forest\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + random_forest\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 7.2855 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 0.5548 seconds\n",
            "â±ï¸  Average Time per Sample: 0.2376 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 37.2894 ms\n",
            "â±ï¸  Single Prediction Time (min): 36.0146 ms\n",
            "â±ï¸  Single Prediction Time (max): 39.0472 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.98      1.00      0.99      2105\n",
            "        Judi       0.95      0.82      0.88       230\n",
            "\n",
            "    accuracy                           0.98      2335\n",
            "   macro avg       0.97      0.91      0.93      2335\n",
            "weighted avg       0.98      0.98      0.98      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2096    9]\n",
            " [  42  188]]\n",
            "\n",
            "Accuracy: 0.9782\n",
            "Precision: 0.9543\n",
            "Recall: 0.8174\n",
            "F1-Score: 0.8806\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 39.3130 seconds (0.66 minutes)\n",
            "Cross-Validation F1-Score: 0.7806 (+/- 0.1929)\n",
            "\n",
            "[12/18] Testing: hybrid_word_char + xgboost\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + xgboost\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 29.9234 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 0.7275 seconds\n",
            "â±ï¸  Average Time per Sample: 0.3116 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 3.2501 ms\n",
            "â±ï¸  Single Prediction Time (min): 2.6290 ms\n",
            "â±ï¸  Single Prediction Time (max): 10.8104 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.99      0.87      0.92       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.99      0.93      0.96      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2103    2]\n",
            " [  31  199]]\n",
            "\n",
            "Accuracy: 0.9859\n",
            "Precision: 0.9900\n",
            "Recall: 0.8652\n",
            "F1-Score: 0.9234\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 143.6058 seconds (2.39 minutes)\n",
            "Cross-Validation F1-Score: 0.8025 (+/- 0.2380)\n",
            "\n",
            "[13/18] Testing: hybrid_word_char + svm_linear\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + svm_linear\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 62.8879 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 3.1671 seconds\n",
            "â±ï¸  Average Time per Sample: 1.3563 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 3.8867 ms\n",
            "â±ï¸  Single Prediction Time (min): 3.5362 ms\n",
            "â±ï¸  Single Prediction Time (max): 5.8331 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      1.00      2105\n",
            "        Judi       0.97      0.94      0.96       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.98      0.97      0.98      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2099    6]\n",
            " [  14  216]]\n",
            "\n",
            "Accuracy: 0.9914\n",
            "Precision: 0.9730\n",
            "Recall: 0.9391\n",
            "F1-Score: 0.9558\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 324.7219 seconds (5.41 minutes)\n",
            "Cross-Validation F1-Score: 0.8724 (+/- 0.1530)\n",
            "\n",
            "[14/18] Testing: hybrid_word_char + svm_rbf\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + svm_rbf\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 150.2684 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 6.1679 seconds\n",
            "â±ï¸  Average Time per Sample: 2.6415 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 10.1778 ms\n",
            "â±ï¸  Single Prediction Time (min): 6.7422 ms\n",
            "â±ï¸  Single Prediction Time (max): 15.0630 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      1.00      2105\n",
            "        Judi       1.00      0.92      0.96       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       1.00      0.96      0.98      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2105    0]\n",
            " [  19  211]]\n",
            "\n",
            "Accuracy: 0.9919\n",
            "Precision: 1.0000\n",
            "Recall: 0.9174\n",
            "F1-Score: 0.9569\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 770.6903 seconds (12.84 minutes)\n",
            "Cross-Validation F1-Score: 0.8389 (+/- 0.2004)\n",
            "\n",
            "[15/18] Testing: hybrid_word_char + naive_bayes\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + naive_bayes\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 3.3583 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 0.4690 seconds\n",
            "â±ï¸  Average Time per Sample: 0.2008 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 2.3275 ms\n",
            "â±ï¸  Single Prediction Time (min): 1.9646 ms\n",
            "â±ï¸  Single Prediction Time (max): 3.6981 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.97      0.88      0.92       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.98      0.94      0.96      2335\n",
            "weighted avg       0.98      0.99      0.98      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2098    7]\n",
            " [  28  202]]\n",
            "\n",
            "Accuracy: 0.9850\n",
            "Precision: 0.9665\n",
            "Recall: 0.8783\n",
            "F1-Score: 0.9203\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 21.3439 seconds (0.36 minutes)\n",
            "Cross-Validation F1-Score: 0.8610 (+/- 0.0916)\n",
            "\n",
            "[16/18] Testing: hybrid_word_char + knn\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + knn\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 3.2929 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 2.8646 seconds\n",
            "â±ï¸  Average Time per Sample: 1.2268 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 43.0265 ms\n",
            "â±ï¸  Single Prediction Time (min): 34.0073 ms\n",
            "â±ï¸  Single Prediction Time (max): 75.2990 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.94      1.00      0.97      2105\n",
            "        Judi       0.99      0.41      0.58       230\n",
            "\n",
            "    accuracy                           0.94      2335\n",
            "   macro avg       0.96      0.71      0.78      2335\n",
            "weighted avg       0.94      0.94      0.93      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2104    1]\n",
            " [ 135   95]]\n",
            "\n",
            "Accuracy: 0.9418\n",
            "Precision: 0.9896\n",
            "Recall: 0.4130\n",
            "F1-Score: 0.5828\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 31.2692 seconds (0.52 minutes)\n",
            "Cross-Validation F1-Score: 0.4707 (+/- 0.1393)\n",
            "\n",
            "[17/18] Testing: hybrid_word_char + lightgbm\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + lightgbm\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 9.5430 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 0.5238 seconds\n",
            "â±ï¸  Average Time per Sample: 0.2243 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 23.1652 ms\n",
            "â±ï¸  Single Prediction Time (min): 11.0023 ms\n",
            "â±ï¸  Single Prediction Time (max): 994.4463 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.98      1.00      0.99      2105\n",
            "        Judi       0.98      0.84      0.91       230\n",
            "\n",
            "    accuracy                           0.98      2335\n",
            "   macro avg       0.98      0.92      0.95      2335\n",
            "weighted avg       0.98      0.98      0.98      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2102    3]\n",
            " [  36  194]]\n",
            "\n",
            "Accuracy: 0.9833\n",
            "Precision: 0.9848\n",
            "Recall: 0.8435\n",
            "F1-Score: 0.9087\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 50.2899 seconds (0.84 minutes)\n",
            "Cross-Validation F1-Score: 0.7927 (+/- 0.2259)\n",
            "\n",
            "[18/18] Testing: hybrid_word_char + gradient_boosting\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + gradient_boosting\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 102.7772 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 0.4868 seconds\n",
            "â±ï¸  Average Time per Sample: 0.2085 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 2.5861 ms\n",
            "â±ï¸  Single Prediction Time (min): 2.1935 ms\n",
            "â±ï¸  Single Prediction Time (max): 4.2508 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.95      0.92      0.94       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.97      0.96      0.96      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2095   10]\n",
            " [  19  211]]\n",
            "\n",
            "Accuracy: 0.9876\n",
            "Precision: 0.9548\n",
            "Recall: 0.9174\n",
            "F1-Score: 0.9357\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 515.3421 seconds (8.59 minutes)\n",
            "Cross-Validation F1-Score: 0.8058 (+/- 0.2532)\n",
            "\n",
            "================================================================================\n",
            "RANKING BERDASARKAN F1-SCORE\n",
            "================================================================================\n",
            "          vectorizer           classifier  precision    recall  f1_score  \\\n",
            "13  hybrid_word_char              svm_rbf   1.000000  0.917391  0.956916   \n",
            "12  hybrid_word_char           svm_linear   0.972973  0.939130  0.955752   \n",
            "3         tfidf_char           svm_linear   0.944206  0.956522  0.950324   \n",
            "9   hybrid_word_char  logistic_regression   0.964126  0.934783  0.949227   \n",
            "7         tfidf_char             lightgbm   0.990521  0.908696  0.947846   \n",
            "4         tfidf_char              svm_rbf   0.990476  0.904348  0.945455   \n",
            "2         tfidf_char              xgboost   0.990338  0.891304  0.938215   \n",
            "0         tfidf_char  logistic_regression   0.938865  0.934783  0.936819   \n",
            "17  hybrid_word_char    gradient_boosting   0.954751  0.917391  0.935698   \n",
            "8         tfidf_char    gradient_boosting   0.980769  0.886957  0.931507   \n",
            "\n",
            "    single_pred_ms  \n",
            "13       10.177784  \n",
            "12        3.886681  \n",
            "3         2.834647  \n",
            "9         2.164071  \n",
            "7        13.091395  \n",
            "4         4.975564  \n",
            "2         2.101884  \n",
            "0         0.977063  \n",
            "17        2.586095  \n",
            "8         1.310639  \n",
            "\n",
            "================================================================================\n",
            "RANKING BERDASARKAN KECEPATAN PREDIKSI (FASTEST)\n",
            "================================================================================\n",
            "          vectorizer           classifier  precision    recall  f1_score  \\\n",
            "5         tfidf_char          naive_bayes   0.973118  0.786957  0.870192   \n",
            "0         tfidf_char  logistic_regression   0.938865  0.934783  0.936819   \n",
            "8         tfidf_char    gradient_boosting   0.980769  0.886957  0.931507   \n",
            "2         tfidf_char              xgboost   0.990338  0.891304  0.938215   \n",
            "9   hybrid_word_char  logistic_regression   0.964126  0.934783  0.949227   \n",
            "14  hybrid_word_char          naive_bayes   0.966507  0.878261  0.920273   \n",
            "17  hybrid_word_char    gradient_boosting   0.954751  0.917391  0.935698   \n",
            "3         tfidf_char           svm_linear   0.944206  0.956522  0.950324   \n",
            "11  hybrid_word_char              xgboost   0.990050  0.865217  0.923434   \n",
            "12  hybrid_word_char           svm_linear   0.972973  0.939130  0.955752   \n",
            "\n",
            "    single_pred_ms  \n",
            "5         0.940270  \n",
            "0         0.977063  \n",
            "8         1.310639  \n",
            "2         2.101884  \n",
            "9         2.164071  \n",
            "14        2.327461  \n",
            "17        2.586095  \n",
            "3         2.834647  \n",
            "11        3.250072  \n",
            "12        3.886681  \n",
            "\n",
            "================================================================================\n",
            "HASIL PERBANDINGAN MODEL (Sorted by F1-Score)\n",
            "================================================================================\n",
            "      vectorizer          classifier  accuracy  precision   recall  f1_score  cv_f1_mean  cv_f1_std  train_time_sec  single_pred_ms  single_pred_min_ms  single_pred_max_ms  cv_time_min\n",
            "hybrid_word_char             svm_rbf  0.991863   1.000000 0.917391  0.956916    0.838919   0.200354      150.268370       10.177784            6.742239           15.063047    12.844838\n",
            "hybrid_word_char          svm_linear  0.991435   0.972973 0.939130  0.955752    0.872398   0.153005       62.887941        3.886681            3.536224            5.833149     5.412032\n",
            "      tfidf_char          svm_linear  0.990150   0.944206 0.956522  0.950324    0.883182   0.142825       68.785555        2.834647            2.496958            4.712343     5.919740\n",
            "hybrid_word_char logistic_regression  0.990150   0.964126 0.934783  0.949227    0.891296   0.098345        4.308781        2.164071            1.908779            3.485680     0.349475\n",
            "      tfidf_char            lightgbm  0.990150   0.990521 0.908696  0.947846    0.820867   0.262328       10.040376       13.091395            8.071423          235.364199     0.864587\n",
            "      tfidf_char             svm_rbf  0.989722   0.990476 0.904348  0.945455    0.844258   0.209963      130.293381        4.975564            4.552126            5.962372    11.239613\n",
            "      tfidf_char             xgboost  0.988437   0.990338 0.891304  0.938215    0.817803   0.261348       31.961616        2.101884            1.486063            7.114887     2.405491\n",
            "      tfidf_char logistic_regression  0.987580   0.938865 0.934783  0.936819    0.888768   0.111537        3.889606        0.977063            0.865698            2.085924     0.280866\n",
            "hybrid_word_char   gradient_boosting  0.987580   0.954751 0.917391  0.935698    0.805760   0.253164      102.777230        2.586095            2.193451            4.250765     8.589034\n",
            "      tfidf_char   gradient_boosting  0.987152   0.980769 0.886957  0.931507    0.816654   0.257931      109.558513        1.310639            1.199484            2.530336     9.212768\n",
            "hybrid_word_char             xgboost  0.985867   0.990050 0.865217  0.923434    0.802463   0.238047       29.923387        3.250072            2.629042           10.810375     2.393430\n",
            "hybrid_word_char         naive_bayes  0.985011   0.966507 0.878261  0.920273    0.860996   0.091627        3.358254        2.327461            1.964569            3.698111     0.355732\n",
            "hybrid_word_char            lightgbm  0.983298   0.984772 0.843478  0.908665    0.792685   0.225918        9.543047       23.165247           11.002302          994.446278     0.838165\n",
            "      tfidf_char       random_forest  0.981585   0.969849 0.839130  0.899767    0.805510   0.196060        6.170224       34.455714           24.842978           51.868439     0.577028\n",
            "hybrid_word_char       random_forest  0.978158   0.954315 0.817391  0.880562    0.780639   0.192860        7.285459       37.289393           36.014557           39.047241     0.655217\n",
            "      tfidf_char         naive_bayes  0.976874   0.973118 0.786957  0.870192    0.791611   0.142049        2.632657        0.940270            0.883579            1.773834     0.268369\n",
            "      tfidf_char                 knn  0.950321   0.991379 0.500000  0.664740    0.556557   0.203099        2.572119       39.372914           33.752680           89.793682     0.449408\n",
            "hybrid_word_char                 knn  0.941756   0.989583 0.413043  0.582822    0.470688   0.139283        3.292913       43.026533           34.007311           75.299025     0.521154\n",
            "\n",
            "================================================================================\n",
            "TOP 5 BEST MODELS\n",
            "================================================================================\n",
            "      vectorizer          classifier  accuracy  precision   recall  f1_score  cv_f1_mean  cv_f1_std  train_time_sec  single_pred_ms  single_pred_min_ms  single_pred_max_ms  cv_time_min\n",
            "hybrid_word_char             svm_rbf  0.991863   1.000000 0.917391  0.956916    0.838919   0.200354      150.268370       10.177784            6.742239           15.063047    12.844838\n",
            "hybrid_word_char          svm_linear  0.991435   0.972973 0.939130  0.955752    0.872398   0.153005       62.887941        3.886681            3.536224            5.833149     5.412032\n",
            "      tfidf_char          svm_linear  0.990150   0.944206 0.956522  0.950324    0.883182   0.142825       68.785555        2.834647            2.496958            4.712343     5.919740\n",
            "hybrid_word_char logistic_regression  0.990150   0.964126 0.934783  0.949227    0.891296   0.098345        4.308781        2.164071            1.908779            3.485680     0.349475\n",
            "      tfidf_char            lightgbm  0.990150   0.990521 0.908696  0.947846    0.820867   0.262328       10.040376       13.091395            8.071423          235.364199     0.864587\n",
            "\n",
            "============================================================\n",
            "AVAILABLE CONFIGURATIONS\n",
            "============================================================\n",
            "\n",
            "ğŸ“Š Available Vectorizers:\n",
            "  1. tfidf_char\n",
            "  2. tfidf_word\n",
            "  3. tfidf_char_wb\n",
            "  4. count_char\n",
            "  5. count_word\n",
            "  6. hashing_char\n",
            "  7. word2vec_cbow\n",
            "  8. word2vec_skipgram\n",
            "  9. fasttext_cbow\n",
            "  10. fasttext_skipgram\n",
            "  11. hybrid_word_char\n",
            "  12. hybrid_all_features\n",
            "\n",
            "ğŸ¤– Available Classifiers:\n",
            "  1. logistic_regression\n",
            "  2. random_forest\n",
            "  3. decision_tree\n",
            "  4. gradient_boosting\n",
            "  5. xgboost\n",
            "  6. lightgbm\n",
            "  7. svm_linear\n",
            "  8. svm_rbf\n",
            "  9. naive_bayes\n",
            "  10. knn\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================\n",
        "# OPSI 1: Test Pipeline Quick Presets\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 1: QUICK PRESETS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "pipeline1 = create_pipeline_char_tfidf()\n",
        "model1, metrics1 = train_and_evaluate(X, y, pipeline1, \"Character TF-IDF\")\n",
        "\n",
        "# ============================================================\n",
        "# OPSI 2: Test Single Custom Pipeline\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 2: CUSTOM PIPELINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Contoh: FastText + Random Forest\n",
        "custom_pipeline = create_custom_pipeline('fasttext_cbow', 'random_forest')\n",
        "model_custom, metrics_custom = train_and_evaluate(\n",
        "    X, y, custom_pipeline,\n",
        "    \"FastText CBOW + Random Forest\"\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# OPSI 3: Compare Multiple Models\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 3: PERBANDINGAN MULTIPLE MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Pilih vectorizer dan classifier yang ingin dibandingkan\n",
        "vectorizers_to_test = [\n",
        "    'tfidf_char',\n",
        "    'hybrid_word_char'\n",
        "]\n",
        "\n",
        "classifiers_to_test = [\n",
        "    'logistic_regression',\n",
        "    'random_forest',\n",
        "    'xgboost',\n",
        "    'svm_linear',\n",
        "    'svm_rbf',\n",
        "    'naive_bayes',\n",
        "    'knn',\n",
        "    'lightgbm',\n",
        "    'gradient_boosting'\n",
        "]\n",
        "\n",
        "# Jalankan perbandingan\n",
        "comparison_results = compare_multiple_models(\n",
        "    X, y,\n",
        "    vectorizers_to_test,\n",
        "    classifiers_to_test\n",
        ")\n",
        "\n",
        "    # Tampilkan hasil\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HASIL PERBANDINGAN MODEL (Sorted by F1-Score)\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_results.to_string(index=False))\n",
        "\n",
        "    # Top 5 models\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TOP 5 BEST MODELS\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_results.head().to_string(index=False))\n",
        "\n",
        "    # ============================================================\n",
        "    # OPSI 4: List Available Configurations\n",
        "    # ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"AVAILABLE CONFIGURATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nğŸ“Š Available Vectorizers:\")\n",
        "for i, vec in enumerate(get_vectorizers().keys(), 1):\n",
        "    print(f\"  {i}. {vec}\")\n",
        "\n",
        "print(\"\\nğŸ¤– Available Classifiers:\")\n",
        "for i, clf in enumerate(get_classifiers().keys(), 1):\n",
        "    print(f\"  {i}. {clf}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdDru7OSq6aD",
        "outputId": "f269df35-8570-4f02-8f50-19dab080d4af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "OPSI 1: QUICK PRESETS\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EVALUASI: Character TF-IDF\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      0.99      0.99      2105\n",
            "        Judi       0.94      0.93      0.94       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.97      0.96      0.96      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2091   14]\n",
            " [  15  215]]\n",
            "\n",
            "Accuracy: 0.9876\n",
            "F1-Score: 0.9368\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.8888 (+/- 0.1115)\n",
            "\n",
            "============================================================\n",
            "OPSI 2: CUSTOM PIPELINE\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EVALUASI: FastText CBOW + Random Forest\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.96      0.98      0.97      2105\n",
            "        Judi       0.81      0.60      0.69       230\n",
            "\n",
            "    accuracy                           0.95      2335\n",
            "   macro avg       0.88      0.79      0.83      2335\n",
            "weighted avg       0.94      0.95      0.94      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2073   32]\n",
            " [  93  137]]\n",
            "\n",
            "Accuracy: 0.9465\n",
            "F1-Score: 0.6867\n",
            "\n",
            "Performing cross-validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-Validation F1-Score: 0.6468 (+/- 0.1785)\n",
            "\n",
            "============================================================\n",
            "OPSI 3: PERBANDINGAN MULTIPLE MODELS\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "MEMULAI PERBANDINGAN 2 KOMBINASI MODEL\n",
            "============================================================\n",
            "\n",
            "\n",
            "[1/2] Testing: hybrid_word_char + logistic_regression\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + logistic_regression\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.96      0.93      0.95       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.98      0.97      0.97      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2097    8]\n",
            " [  15  215]]\n",
            "\n",
            "Accuracy: 0.9901\n",
            "F1-Score: 0.9492\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.8913 (+/- 0.0983)\n",
            "\n",
            "[2/2] Testing: hybrid_all_features + logistic_regression\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_all_features + logistic_regression\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       1.00      1.00      1.00      2105\n",
            "        Judi       0.97      0.96      0.96       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.98      0.98      0.98      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2097    8]\n",
            " [   9  221]]\n",
            "\n",
            "Accuracy: 0.9927\n",
            "F1-Score: 0.9630\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.9043 (+/- 0.1181)\n",
            "\n",
            "================================================================================\n",
            "HASIL PERBANDINGAN MODEL (Sorted by F1-Score)\n",
            "================================================================================\n",
            "         vectorizer          classifier  accuracy  f1_score  cv_f1_mean  cv_f1_std\n",
            "hybrid_all_features logistic_regression  0.992719  0.962963    0.904330   0.118138\n",
            "   hybrid_word_char logistic_regression  0.990150  0.949227    0.891296   0.098345\n",
            "\n",
            "================================================================================\n",
            "TOP 5 BEST MODELS\n",
            "================================================================================\n",
            "         vectorizer          classifier  accuracy  f1_score  cv_f1_mean  cv_f1_std\n",
            "hybrid_all_features logistic_regression  0.992719  0.962963    0.904330   0.118138\n",
            "   hybrid_word_char logistic_regression  0.990150  0.949227    0.891296   0.098345\n",
            "\n",
            "============================================================\n",
            "AVAILABLE CONFIGURATIONS\n",
            "============================================================\n",
            "\n",
            "ğŸ“Š Available Vectorizers:\n",
            "  1. tfidf_char\n",
            "  2. tfidf_word\n",
            "  3. tfidf_char_wb\n",
            "  4. count_char\n",
            "  5. count_word\n",
            "  6. hashing_char\n",
            "  7. word2vec_cbow\n",
            "  8. word2vec_skipgram\n",
            "  9. fasttext_cbow\n",
            "  10. fasttext_skipgram\n",
            "  11. hybrid_word_char\n",
            "  12. hybrid_all_features\n",
            "\n",
            "ğŸ¤– Available Classifiers:\n",
            "  1. logistic_regression\n",
            "  2. random_forest\n",
            "  3. decision_tree\n",
            "  4. gradient_boosting\n",
            "  5. svm_linear\n",
            "  6. svm_rbf\n",
            "  7. naive_bayes\n",
            "  8. knn\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================\n",
        "# OPSI 1: Test Pipeline Quick Presets\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 1: QUICK PRESETS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "pipeline1 = create_pipeline_char_tfidf()\n",
        "model1, metrics1 = train_and_evaluate(X, y, pipeline1, \"Character TF-IDF\")\n",
        "\n",
        "# ============================================================\n",
        "# OPSI 2: Test Single Custom Pipeline\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 2: CUSTOM PIPELINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Contoh: FastText + Random Forest\n",
        "custom_pipeline = create_custom_pipeline('fasttext_cbow', 'random_forest')\n",
        "model_custom, metrics_custom = train_and_evaluate(\n",
        "    X, y, custom_pipeline,\n",
        "    \"FastText CBOW + Random Forest\"\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# OPSI 3: Compare Multiple Models\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 3: PERBANDINGAN MULTIPLE MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Pilih vectorizer dan classifier yang ingin dibandingkan\n",
        "vectorizers_to_test = [\n",
        "    # 'tfidf_char',\n",
        "    'hybrid_word_char',\n",
        "    'hybrid_all_features'\n",
        "]\n",
        "\n",
        "classifiers_to_test = [\n",
        "    'logistic_regression',\n",
        "    # 'svm_rbf',\n",
        "]\n",
        "\n",
        "# Jalankan perbandingan\n",
        "comparison_results = compare_multiple_models(\n",
        "    X, y,\n",
        "    vectorizers_to_test,\n",
        "    classifiers_to_test\n",
        ")\n",
        "\n",
        "    # Tampilkan hasil\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HASIL PERBANDINGAN MODEL (Sorted by F1-Score)\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_results.to_string(index=False))\n",
        "\n",
        "    # Top 5 models\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TOP 5 BEST MODELS\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_results.head().to_string(index=False))\n",
        "\n",
        "    # ============================================================\n",
        "    # OPSI 4: List Available Configurations\n",
        "    # ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"AVAILABLE CONFIGURATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nğŸ“Š Available Vectorizers:\")\n",
        "for i, vec in enumerate(get_vectorizers().keys(), 1):\n",
        "    print(f\"  {i}. {vec}\")\n",
        "\n",
        "print(\"\\nğŸ¤– Available Classifiers:\")\n",
        "for i, clf in enumerate(get_classifiers().keys(), 1):\n",
        "    print(f\"  {i}. {clf}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCk87C2sO9R4",
        "outputId": "3881ec03-853b-48f5-ff76-f2eabce702f1"
      },
      "outputs": [
        {
          "ename": "URLError",
          "evalue": "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1028)>",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mSSLCertVerificationError\u001b[39m                  Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:1319\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m     \u001b[43mh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTransfer-encoding\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1338\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1337\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1384\u001b[39m, in \u001b[36mHTTPConnection._send_request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1383\u001b[39m     body = _encode(body, \u001b[33m'\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1384\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1333\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1333\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1093\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1096\u001b[39m \n\u001b[32m   1097\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1037\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1479\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1477\u001b[39m     server_hostname = \u001b[38;5;28mself\u001b[39m.host\n\u001b[32m-> \u001b[39m\u001b[32m1479\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1480\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:455\u001b[39m, in \u001b[36mSSLContext.wrap_socket\u001b[39m\u001b[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    450\u001b[39m                 do_handshake_on_connect=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    451\u001b[39m                 suppress_ragged_eofs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    452\u001b[39m                 server_hostname=\u001b[38;5;28;01mNone\u001b[39;00m, session=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msslsocket_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1076\u001b[39m, in \u001b[36mSSLSocket._create\u001b[39m\u001b[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[39m\n\u001b[32m   1075\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1076\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1372\u001b[39m, in \u001b[36mSSLSocket.do_handshake\u001b[39m\u001b[34m(self, block)\u001b[39m\n\u001b[32m   1371\u001b[39m         \u001b[38;5;28mself\u001b[39m.settimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1372\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
            "\u001b[31mSSLCertVerificationError\u001b[39m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1028)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mURLError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df_train = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttps://raw.githubusercontent.com/nafhanugm/data-mining2/refs/heads/master/dataset/train.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m df_test = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mhttps://raw.githubusercontent.com/nafhanugm/data-mining2/refs/heads/master/dataset/test.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m df_holdout = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mhttps://raw.githubusercontent.com/nafhanugm/data-mining2/refs/heads/master/dataset/holdout.csv\u001b[39m\u001b[33m'\u001b[39m, delimiter=\u001b[33m'\u001b[39m\u001b[33m;\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/pandas/io/common.py:728\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    725\u001b[39m     codecs.lookup_error(errors)\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m ioargs = \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m handle = ioargs.filepath_or_buffer\n\u001b[32m    737\u001b[39m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/pandas/io/common.py:384\u001b[39m, in \u001b[36m_get_filepath_or_buffer\u001b[39m\u001b[34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[32m    383\u001b[39m req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[32m    385\u001b[39m     content_encoding = req.headers.get(\u001b[33m\"\u001b[39m\u001b[33mContent-Encoding\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding == \u001b[33m\"\u001b[39m\u001b[33mgzip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/pandas/io/common.py:289\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[33;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03mthe stdlib.\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrequest\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:189\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, context)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    188\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:489\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    486\u001b[39m     req = meth(req)\n\u001b[32m    488\u001b[39m sys.audit(\u001b[33m'\u001b[39m\u001b[33murllib.Request\u001b[39m\u001b[33m'\u001b[39m, req.full_url, req.data, req.headers, req.get_method())\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[32m    492\u001b[39m meth_name = protocol+\u001b[33m\"\u001b[39m\u001b[33m_response\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:506\u001b[39m, in \u001b[36mOpenerDirector._open\u001b[39m\u001b[34m(self, req, data)\u001b[39m\n\u001b[32m    503\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    505\u001b[39m protocol = req.type\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m                          \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_open\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m    509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:466\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    465\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    468\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:1367\u001b[39m, in \u001b[36mHTTPSHandler.https_open\u001b[39m\u001b[34m(self, req)\u001b[39m\n\u001b[32m   1366\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[32m-> \u001b[39m\u001b[32m1367\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1368\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/urllib/request.py:1322\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1319\u001b[39m         h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[32m   1320\u001b[39m                   encode_chunked=req.has_header(\u001b[33m'\u001b[39m\u001b[33mTransfer-encoding\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m   1321\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[32m   1323\u001b[39m     r = h.getresponse()\n\u001b[32m   1324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
            "\u001b[31mURLError\u001b[39m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1028)>"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv('https://raw.githubusercontent.com/nafhanugm/data-mining2/refs/heads/master/dataset/train.csv')\n",
        "df_test = pd.read_csv('https://raw.githubusercontent.com/nafhanugm/data-mining2/refs/heads/master/dataset/test.csv')\n",
        "df_holdout = pd.read_csv('https://raw.githubusercontent.com/nafhanugm/data-mining2/refs/heads/master/dataset/holdout.csv', delimiter=';')\n",
        "\n",
        "df_all = pd.concat([df_train, df_test], ignore_index=True)\n",
        "X = df_all['comment'].values\n",
        "y = df_all['label'].values\n",
        "\n",
        "print(\"Dataset shape:\", X.shape)\n",
        "print(\"Label distribution:\", np.bincount(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ef452de",
        "outputId": "fb9bdbe0-e6ea-4af3-f0ae-8af4dbeea817"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing GridSearchCV for hybrid_all_features + logistic_regression...\n",
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.4s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.5s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.5s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.7s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.7s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.9s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.6s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.6s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.2s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.4s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.3s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.5s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.6s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.6s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.6s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.7s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.9s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.8s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.5s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.6s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.6s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.8s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.7s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.9s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.6s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.6s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.7s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.9s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.2s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.6s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   4.8s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   5.0s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   5.5s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   5.9s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   4.2s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   4.2s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   6.3s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   6.2s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.8s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.9s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.7s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.5s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   4.6s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   4.5s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   4.9s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   4.3s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   5.4s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   5.6s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   5.4s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   4.5s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.9s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   4.1s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.7s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.3s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   4.3s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   4.1s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   4.1s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   4.6s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.7s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   4.4s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.7s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   4.2s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.7s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.9s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.0s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.9s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.5s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.4s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.6s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.5s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.2s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.1s\n",
            "\n",
            "Best parameters for hybrid_all_features + logistic_regression:\n",
            "{'classifier__C': 10, 'classifier__solver': 'lbfgs', 'vectorizer__char_tfidf__ngram_range': (2, 4), 'vectorizer__word_tfidf__ngram_range': (1, 2)}\n",
            "Best F1-Score: 0.920277182539412\n",
            "\n",
            "Performing GridSearchCV for hybrid_word_char + logistic_regression...\n",
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.2s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.2s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.3s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.5s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.5s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.6s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.2s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.2s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.3s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.3s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.5s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.4s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.6s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.6s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.5s\n",
            "[CV] END classifier__C=0.1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.7s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.6s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.7s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.6s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.6s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.9s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   4.0s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   4.0s\n",
            "[CV] END classifier__C=0.1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   4.0s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.6s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.7s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.4s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.8s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.7s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.7s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.6s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.4s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.5s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   4.1s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.2s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.3s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.7s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.4s\n",
            "[CV] END classifier__C=1, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   4.0s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.8s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.8s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.7s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.5s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.3s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.3s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.5s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.5s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.5s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.7s\n",
            "[CV] END classifier__C=1, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.6s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.6s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.9s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.0s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.2s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   4.2s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   4.1s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   4.3s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   4.4s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.7s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.7s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   4.5s\n",
            "[CV] END classifier__C=10, classifier__solver=liblinear, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   4.1s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   2.6s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.7s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.8s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 4), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.8s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.7s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.3s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 1); total time=   3.2s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   3.4s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.8s\n",
            "[CV] END classifier__C=10, classifier__solver=lbfgs, vectorizer__char_tfidf__ngram_range=(2, 5), vectorizer__word_tfidf__ngram_range=(1, 2); total time=   2.7s\n",
            "\n",
            "Best parameters for hybrid_word_char + logistic_regression:\n",
            "{'classifier__C': 1, 'classifier__solver': 'lbfgs', 'vectorizer__char_tfidf__ngram_range': (2, 5), 'vectorizer__word_tfidf__ngram_range': (1, 1)}\n",
            "Best F1-Score: 0.8976769036432941\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grids for each pipeline\n",
        "param_grid_hybrid_all_lr = {\n",
        "    'vectorizer__word_tfidf__ngram_range': [(1, 1), (1, 2)],\n",
        "    'vectorizer__char_tfidf__ngram_range': [(2, 4), (2, 5)],\n",
        "    'classifier__C': [0.1, 1, 10],\n",
        "    'classifier__solver': ['liblinear', 'lbfgs']\n",
        "}\n",
        "\n",
        "param_grid_hybrid_word_char_lr = {\n",
        "    'vectorizer__word_tfidf__ngram_range': [(1, 1), (1, 2)],\n",
        "    'vectorizer__char_tfidf__ngram_range': [(2, 4), (2, 5)],\n",
        "    'classifier__C': [0.1, 1, 10],\n",
        "    'classifier__solver': ['liblinear', 'lbfgs']\n",
        "}\n",
        "\n",
        "# Create the pipelines\n",
        "pipeline_hybrid_all_lr = create_custom_pipeline('hybrid_all_features', 'logistic_regression')\n",
        "pipeline_hybrid_word_char_lr = create_custom_pipeline('hybrid_word_char', 'logistic_regression')\n",
        "\n",
        "# Perform GridSearchCV for hybrid_all_features + logistic_regression\n",
        "print(\"Performing GridSearchCV for hybrid_all_features + logistic_regression...\")\n",
        "grid_search_hybrid_all_lr = GridSearchCV(\n",
        "    pipeline_hybrid_all_lr,\n",
        "    param_grid_hybrid_all_lr,\n",
        "    cv=3,  # Using 3-fold cross-validation for faster tuning\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "grid_search_hybrid_all_lr.fit(X, y)\n",
        "\n",
        "print(\"\\nBest parameters for hybrid_all_features + logistic_regression:\")\n",
        "print(grid_search_hybrid_all_lr.best_params_)\n",
        "print(\"Best F1-Score:\", grid_search_hybrid_all_lr.best_score_)\n",
        "\n",
        "# Perform GridSearchCV for hybrid_word_char + logistic_regression\n",
        "print(\"\\nPerforming GridSearchCV for hybrid_word_char + logistic_regression...\")\n",
        "grid_search_hybrid_word_char_lr = GridSearchCV(\n",
        "    pipeline_hybrid_word_char_lr,\n",
        "    param_grid_hybrid_word_char_lr,\n",
        "    cv=3,  # Using 3-fold cross-validation for faster tuning\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "grid_search_hybrid_word_char_lr.fit(X, y)\n",
        "\n",
        "print(\"\\nBest parameters for hybrid_word_char + logistic_regression:\")\n",
        "print(grid_search_hybrid_word_char_lr.best_params_)\n",
        "print(\"Best F1-Score:\", grid_search_hybrid_word_char_lr.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCMDv_sINO39"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DnOzulDNV1w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRrkQEdEASBD",
        "outputId": "5334196f-6a4a-44d8-f834-9f8b4cd3abab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "OPSI 1: QUICK PRESETS\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EVALUASI: Character TF-IDF\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      0.99      0.99      2105\n",
            "        Judi       0.94      0.93      0.94       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.97      0.96      0.96      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2091   14]\n",
            " [  15  215]]\n",
            "\n",
            "Accuracy: 0.9876\n",
            "F1-Score: 0.9368\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.8888 (+/- 0.1115)\n",
            "\n",
            "============================================================\n",
            "OPSI 2: CUSTOM PIPELINE\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EVALUASI: FastText CBOW + Random Forest\n",
            "============================================================\n",
            "Training model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.96      0.99      0.97      2105\n",
            "        Judi       0.84      0.61      0.71       230\n",
            "\n",
            "    accuracy                           0.95      2335\n",
            "   macro avg       0.90      0.80      0.84      2335\n",
            "weighted avg       0.95      0.95      0.95      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2078   27]\n",
            " [  90  140]]\n",
            "\n",
            "Accuracy: 0.9499\n",
            "F1-Score: 0.7053\n",
            "\n",
            "Performing cross-validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-Validation F1-Score: 0.6384 (+/- 0.1703)\n",
            "\n",
            "============================================================\n",
            "OPSI 3: PERBANDINGAN MULTIPLE MODELS\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "MEMULAI PERBANDINGAN 3 KOMBINASI MODEL\n",
            "============================================================\n",
            "\n",
            "\n",
            "[1/3] Testing: tfidf_char + logistic_regression\n",
            "\n",
            "============================================================\n",
            "EVALUASI: tfidf_char + logistic_regression\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      0.99      0.99      2105\n",
            "        Judi       0.94      0.93      0.94       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.97      0.96      0.96      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2091   14]\n",
            " [  15  215]]\n",
            "\n",
            "Accuracy: 0.9876\n",
            "F1-Score: 0.9368\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.8888 (+/- 0.1115)\n",
            "\n",
            "[2/3] Testing: hybrid_word_char + logistic_regression\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_word_char + logistic_regression\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.96      0.93      0.95       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.98      0.97      0.97      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2097    8]\n",
            " [  15  215]]\n",
            "\n",
            "Accuracy: 0.9901\n",
            "F1-Score: 0.9492\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.8913 (+/- 0.0983)\n",
            "\n",
            "[3/3] Testing: hybrid_all_features + logistic_regression\n",
            "\n",
            "============================================================\n",
            "EVALUASI: hybrid_all_features + logistic_regression\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       1.00      1.00      1.00      2105\n",
            "        Judi       0.97      0.96      0.96       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.98      0.98      0.98      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2097    8]\n",
            " [   9  221]]\n",
            "\n",
            "Accuracy: 0.9927\n",
            "F1-Score: 0.9630\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.9043 (+/- 0.1181)\n",
            "\n",
            "============================================================\n",
            "OPSI 4: MENAMBAHKAN MODEL TUNED KE PERBANDINGAN\n",
            "============================================================\n",
            "\n",
            "Evaluating Tuned Hybrid All Features + Logistic Regression...\n",
            "\n",
            "============================================================\n",
            "EVALUASI: TUNED Hybrid All Features + Logistic Regression\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       1.00      1.00      1.00      2105\n",
            "        Judi       0.99      0.97      0.98       230\n",
            "\n",
            "    accuracy                           1.00      2335\n",
            "   macro avg       0.99      0.98      0.99      2335\n",
            "weighted avg       1.00      1.00      1.00      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2102    3]\n",
            " [   8  222]]\n",
            "\n",
            "Accuracy: 0.9953\n",
            "F1-Score: 0.9758\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.9164 (+/- 0.1160)\n",
            "\n",
            "Evaluating Tuned Hybrid Word Char + Logistic Regression...\n",
            "\n",
            "============================================================\n",
            "EVALUASI: TUNED Hybrid Word Char + Logistic Regression\n",
            "============================================================\n",
            "Training model...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.99      1.00      0.99      2105\n",
            "        Judi       0.96      0.93      0.95       230\n",
            "\n",
            "    accuracy                           0.99      2335\n",
            "   macro avg       0.98      0.97      0.97      2335\n",
            "weighted avg       0.99      0.99      0.99      2335\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2097    8]\n",
            " [  15  215]]\n",
            "\n",
            "Accuracy: 0.9901\n",
            "F1-Score: 0.9492\n",
            "\n",
            "Performing cross-validation...\n",
            "Cross-Validation F1-Score: 0.8888 (+/- 0.1030)\n",
            "\n",
            "An error occurred while adding tuned models: 'precision'\n",
            "\n",
            "================================================================================\n",
            "HASIL PERBANDINGAN MODEL (Sorted by F1-Score)\n",
            "================================================================================\n",
            "         vectorizer          classifier  accuracy  f1_score  cv_f1_mean  cv_f1_std\n",
            "hybrid_all_features logistic_regression  0.992719  0.962963    0.904330   0.118138\n",
            "   hybrid_word_char logistic_regression  0.990150  0.949227    0.891296   0.098345\n",
            "         tfidf_char logistic_regression  0.987580  0.936819    0.888768   0.111537\n",
            "\n",
            "================================================================================\n",
            "TOP 5 BEST MODELS (Sorted by F1-Score)\n",
            "================================================================================\n",
            "         vectorizer          classifier  accuracy  f1_score  cv_f1_mean  cv_f1_std\n",
            "hybrid_all_features logistic_regression  0.992719  0.962963    0.904330   0.118138\n",
            "   hybrid_word_char logistic_regression  0.990150  0.949227    0.891296   0.098345\n",
            "         tfidf_char logistic_regression  0.987580  0.936819    0.888768   0.111537\n",
            "\n",
            "================================================================================\n",
            "RANKING BERDASARKAN CROSS-VALIDATION F1-SCORE\n",
            "================================================================================\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "\"['precision', 'recall', 'single_pred_ms'] not in index\"",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 132\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRANKING BERDASARKAN CROSS-VALIDATION F1-SCORE\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    131\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcomparison_results\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcv_f1_mean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvectorizer\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclassifier\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprecision\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrecall\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mf1_score\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcv_f1_mean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msingle_pred_ms\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.head(\u001b[32m10\u001b[39m))\n\u001b[32m    134\u001b[39m     \u001b[38;5;66;03m# Display summary sorted by prediction speed\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/penambangan-data/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mKeyError\u001b[39m: \"['precision', 'recall', 'single_pred_ms'] not in index\""
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# OPSI 1: Test Pipeline Quick Presets\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 1: QUICK PRESETS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "pipeline1 = create_pipeline_char_tfidf()\n",
        "model1, metrics1 = train_and_evaluate(X, y, pipeline1, \"Character TF-IDF\")\n",
        "\n",
        "# ============================================================\n",
        "# OPSI 2: Test Single Custom Pipeline\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 2: CUSTOM PIPELINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Contoh: FastText + Random Forest\n",
        "custom_pipeline = create_custom_pipeline('fasttext_cbow', 'random_forest')\n",
        "model_custom, metrics_custom = train_and_evaluate(\n",
        "    X, y, custom_pipeline,\n",
        "    \"FastText CBOW + Random Forest\"\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# OPSI 3: Compare Multiple Models\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 3: PERBANDINGAN MULTIPLE MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Pilih vectorizer dan classifier yang ingin dibandingkan\n",
        "vectorizers_to_test = [\n",
        "    'tfidf_char',\n",
        "    'hybrid_word_char',\n",
        "    'hybrid_all_features'\n",
        "]\n",
        "\n",
        "classifiers_to_test = [\n",
        "    'logistic_regression'\n",
        "]\n",
        "\n",
        "# Jalankan perbandingan\n",
        "comparison_results = compare_multiple_models(\n",
        "    X, y,\n",
        "    vectorizers_to_test,\n",
        "    classifiers_to_test\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# OPSI 4: Add Tuned Models to Comparison\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 4: MENAMBAHKAN MODEL TUNED KE PERBANDINGAN\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get the best estimators from GridSearchCV (assuming they were run in a previous cell)\n",
        "try:\n",
        "    tuned_hybrid_all_lr = grid_search_hybrid_all_lr.best_estimator_\n",
        "    tuned_hybrid_word_char_lr = grid_search_hybrid_word_char_lr.best_estimator_\n",
        "\n",
        "    # Evaluate tuned models\n",
        "    print(\"\\nEvaluating Tuned Hybrid All Features + Logistic Regression...\")\n",
        "    model_tuned_hybrid_all_lr, metrics_tuned_hybrid_all_lr = train_and_evaluate(\n",
        "        X, y, tuned_hybrid_all_lr,\n",
        "        \"TUNED Hybrid All Features + Logistic Regression\"\n",
        "    )\n",
        "\n",
        "    print(\"\\nEvaluating Tuned Hybrid Word Char + Logistic Regression...\")\n",
        "    model_tuned_hybrid_word_char_lr, metrics_tuned_hybrid_word_char_lr = train_and_evaluate(\n",
        "        X, y, tuned_hybrid_word_char_lr,\n",
        "        \"TUNED Hybrid Word Char + Logistic Regression\"\n",
        "    )\n",
        "\n",
        "    # Add tuned model results to the comparison DataFrame\n",
        "    tuned_results = []\n",
        "    tuned_results.append({\n",
        "        'vectorizer': 'hybrid_all_features (TUNED)',\n",
        "        'classifier': 'logistic_regression (TUNED)',\n",
        "        'accuracy': metrics_tuned_hybrid_all_lr['accuracy'],\n",
        "        'precision': metrics_tuned_hybrid_all_lr['precision'],\n",
        "        'recall': metrics_tuned_hybrid_all_lr['recall'],\n",
        "        'f1_score': metrics_tuned_hybrid_all_lr['f1_score'],\n",
        "        'cv_f1_mean': metrics_tuned_hybrid_all_lr['cv_f1_mean'],\n",
        "        'cv_f1_std': metrics_tuned_hybrid_all_lr['cv_f1_std'],\n",
        "        'train_time_sec': metrics_tuned_hybrid_all_lr['train_time_sec'],\n",
        "        'single_pred_ms': metrics_tuned_hybrid_all_lr['single_pred_time_ms'],\n",
        "        'single_pred_min_ms': metrics_tuned_hybrid_all_lr['single_pred_min_ms'],\n",
        "        'single_pred_max_ms': metrics_tuned_hybrid_all_lr['single_pred_max_ms'],\n",
        "        'cv_time_min': metrics_tuned_hybrid_all_lr['cv_time_min']\n",
        "    })\n",
        "    tuned_results.append({\n",
        "        'vectorizer': 'hybrid_word_char (TUNED)',\n",
        "        'classifier': 'logistic_regression (TUNED)',\n",
        "        'accuracy': metrics_tuned_hybrid_word_char_lr['accuracy'],\n",
        "        'precision': metrics_tuned_hybrid_word_char_lr['precision'],\n",
        "        'recall': metrics_tuned_hybrid_word_char_lr['recall'],\n",
        "        'f1_score': metrics_tuned_hybrid_word_char_lr['f1_score'],\n",
        "        'cv_f1_mean': metrics_tuned_hybrid_word_char_lr['cv_f1_mean'],\n",
        "        'cv_f1_std': metrics_tuned_hybrid_word_char_lr['cv_f1_std'],\n",
        "        'train_time_sec': metrics_tuned_hybrid_word_char_lr['train_time_sec'],\n",
        "        'single_pred_ms': metrics_tuned_hybrid_word_char_lr['single_pred_time_ms'],\n",
        "        'single_pred_min_ms': metrics_tuned_hybrid_word_char_lr['single_pred_min_ms'],\n",
        "        'single_pred_max_ms': metrics_tuned_hybrid_word_char_lr['single_pred_max_ms'],\n",
        "        'cv_time_min': metrics_tuned_hybrid_word_char_lr['cv_time_min']\n",
        "    })\n",
        "\n",
        "    comparison_results = pd.concat([comparison_results, pd.DataFrame(tuned_results)], ignore_index=True)\n",
        "\n",
        "except NameError:\n",
        "    print(\"\\nSkipping tuned model comparison: GridSearchCV results not found. Please run the tuning cell first.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred while adding tuned models: {str(e)}\")\n",
        "\n",
        "\n",
        "    # Tampilkan hasil\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HASIL PERBANDINGAN MODEL (Sorted by F1-Score)\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_results.sort_values('f1_score', ascending=False).to_string(index=False))\n",
        "\n",
        "    # Top 5 models (including tuned if added)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TOP 5 BEST MODELS (Sorted by F1-Score)\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_results.sort_values('f1_score', ascending=False).head().to_string(index=False))\n",
        "\n",
        "    # Display summary sorted by Cross-Validation F1-Score\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"RANKING BERDASARKAN CROSS-VALIDATION F1-SCORE\")\n",
        "print(f\"{'='*80}\")\n",
        "print(comparison_results.sort_values('cv_f1_mean', ascending=False)[['vectorizer', 'classifier', 'precision', 'recall', 'f1_score', 'cv_f1_mean', 'single_pred_ms']].head(10))\n",
        "\n",
        "    # Display summary sorted by prediction speed\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"RANKING BERDASARKAN KECEPATAN PREDIKSI (FASTEST)\")\n",
        "print(f\"{'='*80}\")\n",
        "print(comparison_results.sort_values('single_pred_ms')[['vectorizer', 'classifier', 'precision', 'recall', 'f1_score', 'single_pred_ms']].head(10))\n",
        "\n",
        "\n",
        "    # ============================================================\n",
        "    # OPSI 5: List Available Configurations (Moved to end for clarity)\n",
        "    # ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"AVAILABLE CONFIGURATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nğŸ“Š Available Vectorizers:\")\n",
        "for i, vec in enumerate(get_vectorizers().keys(), 1):\n",
        "    print(f\"  {i}. {vec}\")\n",
        "\n",
        "print(\"\\nğŸ¤– Available Classifiers:\")\n",
        "for i, clf in enumerate(get_classifiers().keys(), 1):\n",
        "    print(f\"  {i}. {clf}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d4b4325"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LqnsAJ9mS5LA"
      },
      "outputs": [],
      "source": [
        "X_val = df_holdout['comment'].values\n",
        "y_val = df_holdout['label'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofIEK8-6TULr",
        "outputId": "b0a6e078-bcfb-42ad-9ba6-0a73f71e22e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "OPSI 6: EVALUASI MODEL TUNED PADA DATA HOLDOUT\n",
            "============================================================\n",
            "\n",
            "Evaluating TUNED Hybrid All Features + Logistic Regression on Holdout Data...\n",
            "\n",
            "============================================================\n",
            "EVALUASI: TUNED Hybrid All Features + Logistic Regression (Holdout)\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 0.3134 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 0.0489 seconds\n",
            "â±ï¸  Average Time per Sample: 0.2089 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 2.8740 ms\n",
            "â±ï¸  Single Prediction Time (min): 2.4090 ms\n",
            "â±ï¸  Single Prediction Time (max): 8.1997 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.98      0.99      0.98       189\n",
            "        Judi       0.95      0.91      0.93        45\n",
            "\n",
            "    accuracy                           0.97       234\n",
            "   macro avg       0.97      0.95      0.96       234\n",
            "weighted avg       0.97      0.97      0.97       234\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[187   2]\n",
            " [  4  41]]\n",
            "\n",
            "Accuracy: 0.9744\n",
            "Precision: 0.9535\n",
            "Recall: 0.9111\n",
            "F1-Score: 0.9318\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 2.0250 seconds (0.03 minutes)\n",
            "Cross-Validation F1-Score: 0.9619 (+/- 0.0091)\n",
            "\n",
            "Hasil Evaluasi TUNED Hybrid All Features + Logistic Regression pada Data Holdout:\n",
            "Accuracy: 0.9744\n",
            "Precision: 0.9535\n",
            "Recall: 0.9111\n",
            "F1-Score: 0.9318\n",
            "Single Prediction Time (avg): 2.8740 ms\n",
            "\n",
            "------------------------------------------------------------\n",
            "Evaluating TUNED Hybrid Word Char + Logistic Regression on Holdout Data...\n",
            "\n",
            "============================================================\n",
            "EVALUASI: TUNED Hybrid Word Char + Logistic Regression (Holdout)\n",
            "============================================================\n",
            "Training model...\n",
            "â±ï¸  Training Time: 0.4090 seconds\n",
            "\n",
            "Testing prediction speed...\n",
            "â±ï¸  Batch Prediction Time: 0.0737 seconds\n",
            "â±ï¸  Average Time per Sample: 0.3148 ms\n",
            "\n",
            "Measuring single prediction latency (100 iterations)...\n",
            "â±ï¸  Single Prediction Time (avg): 3.1331 ms\n",
            "â±ï¸  Single Prediction Time (min): 2.6534 ms\n",
            "â±ï¸  Single Prediction Time (max): 5.9240 ms\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non-Judi       0.98      0.99      0.98       189\n",
            "        Judi       0.95      0.91      0.93        45\n",
            "\n",
            "    accuracy                           0.97       234\n",
            "   macro avg       0.97      0.95      0.96       234\n",
            "weighted avg       0.97      0.97      0.97       234\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[187   2]\n",
            " [  4  41]]\n",
            "\n",
            "Accuracy: 0.9744\n",
            "Precision: 0.9535\n",
            "Recall: 0.9111\n",
            "F1-Score: 0.9318\n",
            "\n",
            "Performing cross-validation...\n",
            "â±ï¸  Cross-Validation Time: 1.9003 seconds (0.03 minutes)\n",
            "Cross-Validation F1-Score: 0.9545 (+/- 0.0195)\n",
            "\n",
            "Hasil Evaluasi TUNED Hybrid Word Char + Logistic Regression pada Data Holdout:\n",
            "Accuracy: 0.9744\n",
            "Precision: 0.9535\n",
            "Recall: 0.9111\n",
            "F1-Score: 0.9318\n",
            "Single Prediction Time (avg): 3.1331 ms\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# OPSI 6: Evaluate Best Tuned Model on Holdout Data\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OPSI 6: EVALUASI MODEL TUNED PADA DATA HOLDOUT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    # Assuming grid_search_hybrid_all_lr and grid_search_hybrid_word_char_lr are available from previous tuning\n",
        "    tuned_hybrid_all_lr = grid_search_hybrid_all_lr.best_estimator_\n",
        "    tuned_hybrid_word_char_lr = grid_search_hybrid_word_char_lr.best_estimator_\n",
        "\n",
        "    print(\"\\nEvaluating TUNED Hybrid All Features + Logistic Regression on Holdout Data...\")\n",
        "    model_holdout_all, metrics_holdout_all = train_and_evaluate(\n",
        "        X_val, y_val, tuned_hybrid_all_lr,\n",
        "        \"TUNED Hybrid All Features + Logistic Regression (Holdout)\"\n",
        "    )\n",
        "\n",
        "    print(\"\\nHasil Evaluasi TUNED Hybrid All Features + Logistic Regression pada Data Holdout:\")\n",
        "    print(f\"Accuracy: {metrics_holdout_all['accuracy']:.4f}\")\n",
        "    print(f\"Precision: {metrics_holdout_all['precision']:.4f}\")\n",
        "    print(f\"Recall: {metrics_holdout_all['recall']:.4f}\")\n",
        "    print(f\"F1-Score: {metrics_holdout_all['f1_score']:.4f}\")\n",
        "    print(f\"Single Prediction Time (avg): {metrics_holdout_all['single_pred_time_ms']:.4f} ms\")\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"Evaluating TUNED Hybrid Word Char + Logistic Regression on Holdout Data...\")\n",
        "    model_holdout_word_char, metrics_holdout_word_char = train_and_evaluate(\n",
        "        X_val, y_val, tuned_hybrid_word_char_lr,\n",
        "        \"TUNED Hybrid Word Char + Logistic Regression (Holdout)\"\n",
        "    )\n",
        "\n",
        "    print(\"\\nHasil Evaluasi TUNED Hybrid Word Char + Logistic Regression pada Data Holdout:\")\n",
        "    print(f\"Accuracy: {metrics_holdout_word_char['accuracy']:.4f}\")\n",
        "    print(f\"Precision: {metrics_holdout_word_char['precision']:.4f}\")\n",
        "    print(f\"Recall: {metrics_holdout_word_char['recall']:.4f}\")\n",
        "    print(f\"F1-Score: {metrics_holdout_word_char['f1_score']:.4f}\")\n",
        "    print(f\"Single Prediction Time (avg): {metrics_holdout_word_char['single_pred_time_ms']:.4f} ms\")\n",
        "\n",
        "\n",
        "except NameError:\n",
        "    print(\"\\nSkipping holdout evaluation: Tuned models ('grid_search_hybrid_all_lr' or 'grid_search_hybrid_word_char_lr') not found. Please run the tuning cell first.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during holdout evaluation: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "579e5496",
        "outputId": "5e3301e8-881a-4320-9000-0fb4112b4579"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "MISCLASSIFIED SAMPLES ON HOLDOUT DATA (6 total)\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"misclassified_data\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"comment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"beneran gampang hasilnya nyata banget\",\n          \"nggak gacor\",\n          \"\\ud835\\udc6a\\ud835\\udc75\\ud835\\udc6b88 bosan ser\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "misclassified_data"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-cc2f4d0a-33c7-4d2c-ba6e-6c492256a683\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>label</th>\n",
              "      <th>predicted_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>beneran gampang hasilnya nyata banget</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>604</th>\n",
              "      <td>nggak gacor</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>665</th>\n",
              "      <td>ubur ubur ikan lele ğ™oğ™ğ™ğ™ğ™‰ğ™€ğŸ®zğŸ´ mantap lee</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>668</th>\n",
              "      <td>gicir murah jeppe lsg mendarat</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>930</th>\n",
              "      <td>banget ya muka lu macingÂ² unboxing 3 btw nanam...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1137</th>\n",
              "      <td>ğ‘ªğ‘µğ‘«88 bosan ser</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc2f4d0a-33c7-4d2c-ba6e-6c492256a683')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cc2f4d0a-33c7-4d2c-ba6e-6c492256a683 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cc2f4d0a-33c7-4d2c-ba6e-6c492256a683');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2c2ba4d6-64b8-4319-83db-6176c165f2ac\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2c2ba4d6-64b8-4319-83db-6176c165f2ac')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2c2ba4d6-64b8-4319-83db-6176c165f2ac button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_f26bb742-a22c-4c64-babd-6f8c93e12d54\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('misclassified_data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f26bb742-a22c-4c64-babd-6f8c93e12d54 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('misclassified_data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                comment  label  \\\n",
              "497               beneran gampang hasilnya nyata banget      1   \n",
              "604                                         nggak gacor      0   \n",
              "665           ubur ubur ikan lele ğ™oğ™ğ™ğ™ğ™‰ğ™€ğŸ®zğŸ´ mantap lee      1   \n",
              "668                      gicir murah jeppe lsg mendarat      1   \n",
              "930   banget ya muka lu macingÂ² unboxing 3 btw nanam...      0   \n",
              "1137                                    ğ‘ªğ‘µğ‘«88 bosan ser      1   \n",
              "\n",
              "      predicted_label  \n",
              "497                 0  \n",
              "604                 1  \n",
              "665                 0  \n",
              "668                 0  \n",
              "930                 1  \n",
              "1137                0  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Get predictions from the best tuned model on the holdout data\n",
        "# Assuming tuned_hybrid_all_lr is the best model from previous tuning\n",
        "try:\n",
        "    y_pred_holdout = tuned_hybrid_all_lr.predict(X_val)\n",
        "\n",
        "    # Find misclassified indices\n",
        "    misclassified_indices = np.where(y_val != y_pred_holdout)[0]\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"MISCLASSIFIED SAMPLES ON HOLDOUT DATA ({len(misclassified_indices)} total)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Display misclassified samples\n",
        "    misclassified_data = df_holdout.iloc[misclassified_indices].copy()\n",
        "    misclassified_data['predicted_label'] = y_pred_holdout[misclassified_indices]\n",
        "\n",
        "    display(misclassified_data)\n",
        "\n",
        "except NameError:\n",
        "    print(\"\\nSkipping misspredict data display: Tuned model ('tuned_hybrid_all_lr') not found. Please run the tuning cell first.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred while displaying misspredict data: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "EXPORT TUNED HYBRID_ALL_FEATURES & LOGISTIC_REGRESSION\n",
            "============================================================\n",
            "\n",
            "âŒ Error: 'grid_search_hybrid_all_lr' not found.\n",
            "   Please run the GridSearchCV cell (Cell 15) first to tune the model.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# EXPORT TUNED MODEL DAN VECTORIZER KE JOBLIB\n",
        "# ============================================================\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPORT TUNED HYBRID_ALL_FEATURES & LOGISTIC_REGRESSION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    # Get the best tuned model from GridSearchCV\n",
        "    tuned_pipeline = grid_search_hybrid_all_lr.best_estimator_\n",
        "    \n",
        "    # Retrain on all available data for final model\n",
        "    print(\"\\nğŸ“Š Retraining tuned model on full dataset...\")\n",
        "    tuned_pipeline.fit(X, y)\n",
        "    print(\"âœ… Model retrained successfully!\")\n",
        "    \n",
        "    # Extract vectorizer and classifier from the pipeline\n",
        "    print(\"\\nğŸ” Extracting vectorizer and classifier from pipeline...\")\n",
        "    tuned_vectorizer = tuned_pipeline.named_steps['vectorizer']\n",
        "    tuned_classifier = tuned_pipeline.named_steps['classifier']\n",
        "    \n",
        "    print(f\"âœ… Vectorizer type: {type(tuned_vectorizer).__name__}\")\n",
        "    print(f\"âœ… Classifier type: {type(tuned_classifier).__name__}\")\n",
        "    \n",
        "    # Create models directory if it doesn't exist\n",
        "    os.makedirs('models', exist_ok=True)\n",
        "    \n",
        "    # Export vectorizer to joblib\n",
        "    vectorizer_path = 'models/hybrid_all_features_tuned.joblib'\n",
        "    print(f\"\\nğŸ’¾ Exporting vectorizer to {vectorizer_path}...\")\n",
        "    joblib.dump(tuned_vectorizer, vectorizer_path)\n",
        "    print(f\"âœ… Vectorizer exported successfully!\")\n",
        "    \n",
        "    # Export classifier to joblib\n",
        "    classifier_path = 'models/logistic_regression_tuned.joblib'\n",
        "    print(f\"\\nğŸ’¾ Exporting classifier to {classifier_path}...\")\n",
        "    joblib.dump(tuned_classifier, classifier_path)\n",
        "    print(f\"âœ… Classifier exported successfully!\")\n",
        "    \n",
        "    # Also export the complete pipeline for convenience\n",
        "    pipeline_path = 'models/hybrid_all_features_lr_pipeline_tuned.joblib'\n",
        "    print(f\"\\nğŸ’¾ Exporting complete pipeline to {pipeline_path}...\")\n",
        "    joblib.dump(tuned_pipeline, pipeline_path)\n",
        "    print(f\"âœ… Complete pipeline exported successfully!\")\n",
        "    \n",
        "    # Display model information\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EXPORT SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nğŸ“¦ Exported Files:\")\n",
        "    print(f\"  1. Vectorizer: {vectorizer_path}\")\n",
        "    print(f\"  2. Classifier: {classifier_path}\")\n",
        "    print(f\"  3. Complete Pipeline: {pipeline_path}\")\n",
        "    \n",
        "    print(f\"\\nğŸ“Š Model Parameters:\")\n",
        "    print(f\"  Vectorizer: hybrid_all_features (TUNED)\")\n",
        "    print(f\"    - word_tfidf ngram_range: {grid_search_hybrid_all_lr.best_params_['vectorizer__word_tfidf__ngram_range']}\")\n",
        "    print(f\"    - char_tfidf ngram_range: {grid_search_hybrid_all_lr.best_params_['vectorizer__char_tfidf__ngram_range']}\")\n",
        "    print(f\"  Classifier: Logistic Regression (TUNED)\")\n",
        "    print(f\"    - C: {grid_search_hybrid_all_lr.best_params_['classifier__C']}\")\n",
        "    print(f\"    - solver: {grid_search_hybrid_all_lr.best_params_['classifier__solver']}\")\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ Performance:\")\n",
        "    print(f\"  Best CV F1-Score: {grid_search_hybrid_all_lr.best_score_:.4f}\")\n",
        "    \n",
        "    # Verify files were created\n",
        "    print(f\"\\nâœ… Verification:\")\n",
        "    if os.path.exists(vectorizer_path):\n",
        "        file_size = os.path.getsize(vectorizer_path) / (1024 * 1024)  # MB\n",
        "        print(f\"  âœ“ Vectorizer file exists ({file_size:.2f} MB)\")\n",
        "    if os.path.exists(classifier_path):\n",
        "        file_size = os.path.getsize(classifier_path) / (1024 * 1024)  # MB\n",
        "        print(f\"  âœ“ Classifier file exists ({file_size:.2f} MB)\")\n",
        "    if os.path.exists(pipeline_path):\n",
        "        file_size = os.path.getsize(pipeline_path) / (1024 * 1024)  # MB\n",
        "        print(f\"  âœ“ Pipeline file exists ({file_size:.2f} MB)\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"âœ… EXPORT COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Example of how to load the models\n",
        "    print(\"\\nğŸ“– Example usage:\")\n",
        "    print(\"```python\")\n",
        "    print(\"# Load vectorizer\")\n",
        "    print(\"vectorizer = joblib.load('models/hybrid_all_features_tuned.joblib')\")\n",
        "    print(\"# Load classifier\")\n",
        "    print(\"classifier = joblib.load('models/logistic_regression_tuned.joblib')\")\n",
        "    print(\"# Or load complete pipeline\")\n",
        "    print(\"pipeline = joblib.load('models/hybrid_all_features_lr_pipeline_tuned.joblib')\")\n",
        "    print(\"# Predict\")\n",
        "    print(\"X_transformed = vectorizer.transform(['sample text'])\")\n",
        "    print(\"prediction = classifier.predict(X_transformed)\")\n",
        "    print(\"```\")\n",
        "    \n",
        "except NameError:\n",
        "    print(\"\\nâŒ Error: 'grid_search_hybrid_all_lr' not found.\")\n",
        "    print(\"   Please run the GridSearchCV cell (Cell 15) first to tune the model.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Error occurred during export: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
